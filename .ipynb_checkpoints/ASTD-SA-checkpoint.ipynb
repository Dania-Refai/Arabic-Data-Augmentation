{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f566b38",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307cfc6",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for ASTD dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752029f7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c6586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba95086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce6168",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd38bc",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>!!.</td>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>0.014882844,-0.051557414,-0.028316082,0.014168...</td>\n",
       "      <td>0.01946623,-0.010952667,-0.039843258,-0.057320...</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['!!.']</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و #فسسسسسس</td>\n",
       "      <td>NEG</td>\n",
       "      <td>لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...</td>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017838204,0.007064947,-0.03709342,-0.0264731...</td>\n",
       "      <td>0.205765</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>.</td>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>0.026780926,0.009709039,-0.030822175,-0.033138...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.237325</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>.</td>\n",
       "      <td># السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...</td>\n",
       "      <td>0.011575715,-0.0191376,-0.041333534,-0.0137087...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...</td>\n",
       "      <td>['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>في الشوارع.</td>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017580768,-0.0027376027,-0.03825421,-0.04189...</td>\n",
       "      <td>0.390541</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['في', 'الشوارع.']</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...   NEG  \\\n",
       "1          دباسم يوسف عمل برنامج البرنامج و #فسسسسسس   NEG   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   NEG   \n",
       "3  #السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...   POS   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   NEG   \n",
       "\n",
       "                                            new_text   \n",
       "0                                                !!.  \\\n",
       "1  لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...   \n",
       "2                                                  .   \n",
       "3                                                  .   \n",
       "4                                        في الشوارع.   \n",
       "\n",
       "                                            all_text   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...  \\\n",
       "1  دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   \n",
       "3  # السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   \n",
       "\n",
       "                                 original_embbedding   \n",
       "0  0.014882844,-0.051557414,-0.028316082,0.014168...  \\\n",
       "1  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "2  0.026780926,0.009709039,-0.030822175,-0.033138...   \n",
       "3  0.011575715,-0.0191376,-0.041333534,-0.0137087...   \n",
       "4  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "\n",
       "                                      new_embbedding  ecu_similarity   \n",
       "0  0.01946623,-0.010952667,-0.039843258,-0.057320...        0.772223  \\\n",
       "1  0.017838204,0.007064947,-0.03709342,-0.0264731...        0.205765   \n",
       "2  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.237325   \n",
       "3  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.352307   \n",
       "4  0.017580768,-0.0027376027,-0.03825421,-0.04189...        0.390541   \n",
       "\n",
       "   cos_similarity  jacc_similarity   \n",
       "0           0.446         0.037037  \\\n",
       "1           0.929         0.437500   \n",
       "2           0.904         0.000000   \n",
       "3           0.829         0.000000   \n",
       "4           0.834         0.360000   \n",
       "\n",
       "                                          text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      all_text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      new_text_split  bleu_sim_1  bleu_sim_2   \n",
       "0                                            ['!!.']        0.89        0.89  \\\n",
       "1  ['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...        0.14        0.13   \n",
       "2                                              ['.']        0.95        0.95   \n",
       "3                                              ['.']        0.79        0.78   \n",
       "4                                 ['في', 'الشوارع.']        0.92        0.92   \n",
       "\n",
       "   bleu_sim_3  bleu_sim_4  \n",
       "0        0.88        0.88  \n",
       "1        0.12        0.11  \n",
       "2        0.95        0.95  \n",
       "3        0.78        0.77  \n",
       "4        0.92        0.92  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'ASTD'\n",
    "datasetpath = \"Augmented-Dataset/xls/ASTD-Unbalanced-Augmented-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017d22bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECU': 0.33158923031772564,\n",
       " 'COS': 0.8526818791946309,\n",
       " 'JAC': 0.3624915367325659,\n",
       " 'BLEU': 0.3949177274138466}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "all_datasets= []\n",
    "SIM_COFFICIENTS_THRESHOLDS = {'ECU': df[\"ecu_similarity\"].mean(), 'COS':df[\"cos_similarity\"].mean(), 'JAC':df[\"jacc_similarity\"].mean(), 'BLEU':df[\"bleu_sim_1\"].mean()}\n",
    "LABEL_TO_AUGMENT = ['NEG', 'NEUTRAL']\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "SIM_COFFICIENTS_THRESHOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1190f",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3363",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14909da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83535fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b294d0",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58ba",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4ecc7",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d16716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTD-Not-Augmented-all-text\n",
      "ECU-ASTD-Augmented-Test-all-text\n",
      "COS-ASTD-Augmented-Test-all-text\n",
      "JAC-ASTD-Augmented-Test-all-text\n",
      "BLE-ASTD-Augmented-Test-all-text\n",
      "ECU-ASTD-Augmented-Test-new-text\n",
      "COS-ASTD-Augmented-Test-new-text\n",
      "JAC-ASTD-Augmented-Test-new-text\n",
      "BLE-ASTD-Augmented-Test-new-text\n",
      "ECU-ASTD-Not-Augmented-Test-all-text\n",
      "COS-ASTD-Not-Augmented-Test-all-text\n",
      "JAC-ASTD-Not-Augmented-Test-all-text\n",
      "BLE-ASTD-Not-Augmented-Test-all-text\n",
      "ECU-ASTD-Not-Augmented-Test-new-text\n",
      "COS-ASTD-Not-Augmented-Test-new-text\n",
      "JAC-ASTD-Not-Augmented-Test-new-text\n",
      "BLE-ASTD-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05ff5e",
   "metadata": {},
   "source": [
    "### Training and Modeling (Model=aubmindlab/bert-base-arabertv02-twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d1329c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Dataset Name: ASTD-Not-Augmented-all-text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7/40 03:33 < 23:29, 0.02 it/s, Epoch 0.29/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 139\u001b[0m\n\u001b[0;32m    130\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    131\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_init(),\n\u001b[0;32m    132\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# start the training\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Artifacts Saving (Model, Tokenizer, and Configurations)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m inv_label_map \u001b[38;5;241m=\u001b[39m inv_label_map \u001b[38;5;241m=\u001b[39m { v:k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m label_map\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2699\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2702\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_results = []\n",
    "\n",
    "for d in all_datasets:\n",
    "    models_files = []\n",
    "    print (\"*********** Dataset Name: \" + d.name)\n",
    "    selected_dataset = copy.deepcopy(d)\n",
    "    \n",
    "    ## ---> Training \n",
    "        # Preprocessing using the AraBERT Processor\n",
    "    arabic_prep = ArabertPreprocessor(model_name)\n",
    "    selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))\n",
    "    selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)) \n",
    "    \n",
    "    # Check the tokenized sentence length to decide on the maximum sentence length value\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    \n",
    "    #print(\"Training Sentence Lengths: \")\n",
    "    #plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.train[DATA_COLUMN].to_list()],bins=range(0,200,2))\n",
    "    #plt.show()\n",
    "    #\n",
    "    #print(\"Testing Sentence Lengths: \")\n",
    "    #plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.test[DATA_COLUMN].to_list()],bins=range(0,200,2))\n",
    "    #plt.show()\n",
    "    #\n",
    "    ## Deciding the maximum length\n",
    "    max_len = 200\n",
    "    \n",
    "    ## Check how many sequences will be truncated\n",
    "    #print(\"Truncated training sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))\n",
    "    #print(\"Truncated testing sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))\n",
    "    #\n",
    "    ## Creating the Classification Dataset Splits\n",
    "    label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
    "    #print(label_map)\n",
    "    \n",
    "    train_dataset = ClassificationDataset(\n",
    "        selected_dataset.train[DATA_COLUMN].to_list(),\n",
    "        selected_dataset.train[LABEL_COLUMN].to_list(),\n",
    "        model_name,\n",
    "        max_len,\n",
    "        label_map\n",
    "      )\n",
    "    test_dataset = ClassificationDataset(\n",
    "        selected_dataset.test[DATA_COLUMN].to_list(),\n",
    "        selected_dataset.test[LABEL_COLUMN].to_list(),\n",
    "        model_name,\n",
    "        max_len,\n",
    "        label_map\n",
    "      )\n",
    "    \n",
    "    \n",
    "    # Return a pretrained model ready to do classification\n",
    "    def model_init():\n",
    "        return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))\n",
    "    \n",
    "    # Defining Evaluation Metric\n",
    "    # p should be of type EvalPrediction\n",
    "    def compute_metrics(p): \n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        assert len(preds) == len(p.label_ids)\n",
    "        macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "        macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "        macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "        acc = accuracy_score(p.label_ids,preds)\n",
    "        # calculate the ROC and PR\n",
    "        probas = p.predictions[:,1]        \n",
    "        fpr, tpr, _ = roc_curve(p.label_ids, probas)\n",
    "        precision, recall, _ = precision_recall_curve(p.label_ids, probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        # save model \n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        model_name = f\"{d.name}_{metric_value:.2f}_{timestamp}.h5\"\n",
    "        model_path = os.path.join(\"models\", model_name)\n",
    "        model.save(model_path)\n",
    "        models_files.append (model_name)\n",
    "        return {       \n",
    "            'macro_f1' : macro_f1,\n",
    "            'accuracy': acc,\n",
    "            'precision': macro_precision,\n",
    "            'recall':macro_recall,\n",
    "            'fpr':fpr,\n",
    "            'tpr':tpr,\n",
    "            'precision_crv':precision,\n",
    "            'recall_crv':recall,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc\n",
    "        }\n",
    "    \n",
    "    # Defining the Seeding Setter\n",
    "    def set_seed(seed=42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic=True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # binomial confidence interval\n",
    "    def get_ci_97_percent (acc, n):\n",
    "        return 2.17 * sqrt( (acc * (1 - acc)) / n)\n",
    "    \n",
    "    ## ---> Modeling (Regular Training)\n",
    "    # Training parameters | Parameters Reference: https://huggingface.co/docs/transformers/main_classes/trainer#trainingarguments\n",
    "    training_args = TrainingArguments( \n",
    "        output_dir= \"./train\",    \n",
    "        adam_epsilon = 1e-8,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = False, # enable this when using V100 or T4 GPU\n",
    "        per_device_train_batch_size = 64, # up to 64 on 16GB with max len of 128\n",
    "        per_device_eval_batch_size = 128,\n",
    "        gradient_accumulation_steps = 2, # use this to scale batch size without needing more memory\n",
    "        num_train_epochs= 2,\n",
    "        warmup_steps = 0,\n",
    "        do_eval = True,\n",
    "        evaluation_strategy = 'steps',\n",
    "        load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "        metric_for_best_model = 'macro_f1',\n",
    "        greater_is_better = True,\n",
    "        seed = 25\n",
    "      )\n",
    "    \n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    # Trainer Creation\n",
    "    trainer = Trainer(\n",
    "        model = model_init(),\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # start the training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Artifacts Saving (Model, Tokenizer, and Configurations)\n",
    "    inv_label_map = inv_label_map = { v:k for k, v in label_map.items()}\n",
    "    trainer.model.config.label2id = label_map\n",
    "    trainer.model.config.id2label = inv_label_map\n",
    "\n",
    "    #print(\"####################### Start GPU Mointoring ###########################\")\n",
    "    #GPUs = GPU.getGPUs()        \n",
    "    #gpu = GPUs[0]    \n",
    "    #torch.cuda.empty_cache()\n",
    "    #del training_args, trainer\n",
    "    #gc.collect()\n",
    "    #\n",
    "    ##print(\"Initial GPU Usage\")\n",
    "    #gpu_usage() \n",
    "    #torch.cuda.empty_cache()\n",
    "    #gpu_usage()\n",
    "    \n",
    " \n",
    "    # do kfold on the training. Check the perfomance on the test set\n",
    "    kfold_dataset = selected_dataset.train\n",
    "    kfold_dataset.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    # Defing the number of Stratified k-fold splits\n",
    "    kf = StratifiedKFold(\n",
    "        n_splits=5,\n",
    "        shuffle=True,\n",
    "        random_state=123\n",
    "      )\n",
    "    \n",
    "    # Train using cross validation and save the best model at each fold    \n",
    "    fold_best_f1 = 0\n",
    "    best_fold = None\n",
    "    \n",
    "    for fold_num , (train, dev) in enumerate(kf.split(kfold_dataset,kfold_dataset['label'])):\n",
    "       \n",
    "        print(\"**************************Starting Fold Num: \", fold_num,\" **************************\")\n",
    "        \n",
    "        train_dataset = ClassificationDataset(list(kfold_dataset[DATA_COLUMN][train]),\n",
    "                                    list(kfold_dataset[LABEL_COLUMN][train]),\n",
    "                                    model_name,\n",
    "                                    max_len,\n",
    "                                    label_map)\n",
    "        \n",
    "        val_dataset = ClassificationDataset(list(kfold_dataset[DATA_COLUMN][dev]),\n",
    "                                    list(kfold_dataset[LABEL_COLUMN][dev]),\n",
    "                                    model_name,\n",
    "                                    max_len,\n",
    "                                    label_map)\n",
    "        \n",
    "        training_args = TrainingArguments( \n",
    "          output_dir= f\"./train\",    \n",
    "          adam_epsilon = 1e-8,\n",
    "          learning_rate = 2e-5,\n",
    "          fp16 = False,\n",
    "          per_device_train_batch_size = 64,\n",
    "          per_device_eval_batch_size = 128,\n",
    "          gradient_accumulation_steps = 2,\n",
    "          num_train_epochs= 2,\n",
    "          warmup_steps = 0,\n",
    "          do_eval = True,\n",
    "          evaluation_strategy = 'steps',\n",
    "          load_best_model_at_end = True,\n",
    "          metric_for_best_model = 'macro_f1',\n",
    "          greater_is_better = True,\n",
    "          seed = 25\n",
    "        )\n",
    "\n",
    "        set_seed(training_args.seed)\n",
    "    \n",
    "        trainer = Trainer(\n",
    "          model = model_init(),\n",
    "          args = training_args,\n",
    "          train_dataset = train_dataset,\n",
    "          eval_dataset=val_dataset,\n",
    "          compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.model.config.label2id = label_map\n",
    "        trainer.model.config.id2label = inv_label_map\n",
    "        \n",
    "        \n",
    "        trainer.train()\n",
    "        results = trainer.evaluate()\n",
    "        results['Dataset_Name'] = d.name\n",
    "        \n",
    "        if results['eval_macro_f1'] > fold_best_f1:\n",
    "            print('* New Best Model Found!')\n",
    "            fold_best_f1 = results['eval_macro_f1']\n",
    "            best_fold = fold_num\n",
    "        \n",
    "        # +++ add confidence interval calculation for all measures\n",
    "        results['Dataset_Name'] = d.name\n",
    "        results['Fold_No' ] = fold_num\n",
    "        results['ci_macro_f1' ] = get_ci_97_percent (results['eval_macro_f1'], len(d.test))\n",
    "        results['ci_accuracy' ] = get_ci_97_percent (results['eval_accuracy'], len(d.test))\n",
    "        results['ci_precision'] = get_ci_97_percent (results['eval_precision'], len(d.test))\n",
    "        results['ci_recall']    = get_ci_97_percent (results['eval_recall'], len(d.test))\n",
    "        all_results.append(results)\n",
    "              \n",
    "    try:\n",
    "        with open(f\"{d.name}.txt\", \"w\") as file:\n",
    "            for item in models_files:\n",
    "                file.write(str(item) + \"\\n\")\n",
    "        print(\"List exported successfully!\")\n",
    "    except FileNotFoundError:\n",
    "        with open(f\"{d.name}.txt\", \"x\") as file:\n",
    "            for item in models_files:\n",
    "                file.write(str(item) + \"\\n\")\n",
    "        print(\"File created and list exported successfully!\")\n",
    "        \n",
    "     \n",
    "        #print(\"####################### Start GPU Mointoring ###########################\")\n",
    "        #GPUs = GPU.getGPUs()        \n",
    "        #gpu = GPUs[0]         \n",
    "        #gpu_usage() \n",
    "        #torch.cuda.empty_cache()    \n",
    "        #del train_dataset, val_dataset, training_args, trainer\n",
    "        #gc.collect()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ArSarcasem-Not-Augmented-all-text\n",
    "#ECU-ASTD-Augmented-Test-all-text\n",
    "#COS-ASTD-Augmented-Test-all-text\n",
    "#JAC-ASTD-Augmented-Test-all-text\n",
    "#BLE-ASTD-Augmented-Test-all-text\n",
    "#ECU-ASTD-Augmented-Test-new-text\n",
    "#COS-ASTD-Augmented-Test-new-text\n",
    "#JAC-ASTD-Augmented-Test-new-text\n",
    "#BLE-ASTD-Augmented-Test-new-text\n",
    "#ECU-ASTD-Not-Augmented-Test-all-text\n",
    "#COS-ASTD-Not-Augmented-Test-all-text\n",
    "#JAC-ASTD-Not-Augmented-Test-all-text\n",
    "#BLE-ASTD-Not-Augmented-Test-all-text\n",
    "#ECU-ASTD-Not-Augmented-Test-new-text\n",
    "#COS-ASTD-Not-Augmented-Test-new-text\n",
    "#JAC-ASTD-Not-Augmented-Test-new-text\n",
    "#BLE-ASTD-Not-Augmented-Test-new-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9af33",
   "metadata": {},
   "source": [
    "### Export Sentiment ِAnalysis Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainResults = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainResults.to_excel(\"LatestResults/ASTD/ASTD-Results-1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a40f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e85c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c499d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f7f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
