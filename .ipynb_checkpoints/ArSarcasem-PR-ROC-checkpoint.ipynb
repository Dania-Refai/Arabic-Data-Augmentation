{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d873db",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844eb8a",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for ArSarcasem dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cdf40",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c6586b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba95086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f215d0f",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f2dc7",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"@Alito_NBA اتوقع انه بيستمر\"</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.</td>\n",
       "      <td>[بريد] اتوقع انه بيستمر \".</td>\n",
       "      <td>0.016990522,-0.01140931,-0.028880069,-0.041880...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.455610</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['\"@Alito_NBA', 'اتوقع', 'انه', 'بيستمر\"']</td>\n",
       "      <td>['[بريد]', 'اتوقع', 'انه', 'بيستمر', '\".']</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"@KSA24 يعني \"بموافقتنا\" لأن دمشق صايرة موسكو\"</td>\n",
       "      <td>neutral</td>\n",
       "      <td>.</td>\n",
       "      <td>[بريد] يعني \" بموافقتنا \" لأن دمشق صايرة موسكو \".</td>\n",
       "      <td>0.01699051,-0.011409308,-0.028880073,-0.041880...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.455610</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['\"@KSA24', 'يعني', '\"بموافقتنا\"', 'لأن', 'دمش...</td>\n",
       "      <td>['[بريد]', 'يعني', '\"', 'بموافقتنا', '\"', 'لأن...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"RT @alaahmad20: قائد في الحرس يعترف بفقدان ال...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>…</td>\n",
       "      <td>\" RT [مستخدم] : قائد في الحرس يعترف بفقدان الس...</td>\n",
       "      <td>0.021876294,0.006824673,-0.037487492,-0.028356...</td>\n",
       "      <td>0.01758077,-0.002737612,-0.038254194,-0.041899...</td>\n",
       "      <td>0.400617</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>['\"RT', '@alaahmad20:', 'قائد', 'في', 'الحرس',...</td>\n",
       "      <td>['\"', 'RT', '[مستخدم]', ':', 'قائد', 'في', 'ال...</td>\n",
       "      <td>['…']</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شوال الفلوس سويرس مشغول اوى اليومين دول بقناة ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>..</td>\n",
       "      <td>شوال الفلوس سويرس مشغول اوى اليومين دول بقناة ...</td>\n",
       "      <td>0.021415915,0.034925364,-0.036916047,-0.010989...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782683,-0.0437...</td>\n",
       "      <td>0.419766</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['شوال', 'الفلوس', 'سويرس', 'مشغول', 'اوى', 'ا...</td>\n",
       "      <td>['شوال', 'الفلوس', 'سويرس', 'مشغول', 'اوى', 'ا...</td>\n",
       "      <td>['..']</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"الأمين العام للأمم المتحدة: بشار الأسد قتل 30...</td>\n",
       "      <td>negative</td>\n",
       "      <td>منظمة الأمن والتعاون في أوروبا : الحرب مع تنظ...</td>\n",
       "      <td>\" الأمين العام للأمم المتحدة : بشار الأسد قتل ...</td>\n",
       "      <td>0.021876294,0.006824673,-0.037487492,-0.028356...</td>\n",
       "      <td>0.017580774,-0.0027376045,-0.03825421,-0.04189...</td>\n",
       "      <td>0.400617</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>['\"الأمين', 'العام', 'للأمم', 'المتحدة:', 'بشا...</td>\n",
       "      <td>['\"', 'الأمين', 'العام', 'للأمم', 'المتحدة', '...</td>\n",
       "      <td>['منظمة', 'الأمن', 'والتعاون', 'في', 'أوروبا',...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label   \n",
       "0                      \"@Alito_NBA اتوقع انه بيستمر\"   neutral  \\\n",
       "1     \"@KSA24 يعني \"بموافقتنا\" لأن دمشق صايرة موسكو\"   neutral   \n",
       "2  \"RT @alaahmad20: قائد في الحرس يعترف بفقدان ال...   neutral   \n",
       "3  شوال الفلوس سويرس مشغول اوى اليومين دول بقناة ...  negative   \n",
       "4  \"الأمين العام للأمم المتحدة: بشار الأسد قتل 30...  negative   \n",
       "\n",
       "                                            new_text   \n",
       "0                                                  .  \\\n",
       "1                                                  .   \n",
       "2                                                  …   \n",
       "3                                                 ..   \n",
       "4   منظمة الأمن والتعاون في أوروبا : الحرب مع تنظ...   \n",
       "\n",
       "                                            all_text   \n",
       "0                         [بريد] اتوقع انه بيستمر \".  \\\n",
       "1  [بريد] يعني \" بموافقتنا \" لأن دمشق صايرة موسكو \".   \n",
       "2  \" RT [مستخدم] : قائد في الحرس يعترف بفقدان الس...   \n",
       "3  شوال الفلوس سويرس مشغول اوى اليومين دول بقناة ...   \n",
       "4  \" الأمين العام للأمم المتحدة : بشار الأسد قتل ...   \n",
       "\n",
       "                                 original_embbedding   \n",
       "0  0.016990522,-0.01140931,-0.028880069,-0.041880...  \\\n",
       "1  0.01699051,-0.011409308,-0.028880073,-0.041880...   \n",
       "2  0.021876294,0.006824673,-0.037487492,-0.028356...   \n",
       "3  0.021415915,0.034925364,-0.036916047,-0.010989...   \n",
       "4  0.021876294,0.006824673,-0.037487492,-0.028356...   \n",
       "\n",
       "                                      new_embbedding  ecu_similarity   \n",
       "0  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.455610  \\\n",
       "1  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.455610   \n",
       "2  0.01758077,-0.002737612,-0.038254194,-0.041899...        0.400617   \n",
       "3  0.022658788,-0.0036188036,-0.033782683,-0.0437...        0.419766   \n",
       "4  0.017580774,-0.0027376045,-0.03825421,-0.04189...        0.400617   \n",
       "\n",
       "   cos_similarity  jacc_similarity   \n",
       "0           0.762         0.000000  \\\n",
       "1           0.762         0.000000   \n",
       "2           0.824         0.029412   \n",
       "3           0.800         0.000000   \n",
       "4           0.824         0.600000   \n",
       "\n",
       "                                          text_split   \n",
       "0         ['\"@Alito_NBA', 'اتوقع', 'انه', 'بيستمر\"']  \\\n",
       "1  ['\"@KSA24', 'يعني', '\"بموافقتنا\"', 'لأن', 'دمش...   \n",
       "2  ['\"RT', '@alaahmad20:', 'قائد', 'في', 'الحرس',...   \n",
       "3  ['شوال', 'الفلوس', 'سويرس', 'مشغول', 'اوى', 'ا...   \n",
       "4  ['\"الأمين', 'العام', 'للأمم', 'المتحدة:', 'بشا...   \n",
       "\n",
       "                                      all_text_split   \n",
       "0         ['[بريد]', 'اتوقع', 'انه', 'بيستمر', '\".']  \\\n",
       "1  ['[بريد]', 'يعني', '\"', 'بموافقتنا', '\"', 'لأن...   \n",
       "2  ['\"', 'RT', '[مستخدم]', ':', 'قائد', 'في', 'ال...   \n",
       "3  ['شوال', 'الفلوس', 'سويرس', 'مشغول', 'اوى', 'ا...   \n",
       "4  ['\"', 'الأمين', 'العام', 'للأمم', 'المتحدة', '...   \n",
       "\n",
       "                                      new_text_split  bleu_sim_1  bleu_sim_2   \n",
       "0                                              ['.']        0.40        0.32  \\\n",
       "1                                              ['.']        0.40        0.30   \n",
       "2                                              ['…']        0.48        0.47   \n",
       "3                                             ['..']        0.95        0.95   \n",
       "4  ['منظمة', 'الأمن', 'والتعاون', 'في', 'أوروبا',...        0.06        0.05   \n",
       "\n",
       "   bleu_sim_3  bleu_sim_4  \n",
       "0        0.00        0.00  \n",
       "1        0.22        0.00  \n",
       "2        0.46        0.44  \n",
       "3        0.94        0.94  \n",
       "4        0.05        0.04  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'ArSarcasem'\n",
    "datasetpath = \"Augmented-Dataset/xls/ArSarcasm-Unbalanced-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd472133",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79e1190f",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3363",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14909da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83535fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b294d0",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58ba",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59578df",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d16716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArSarcasem-Not-Augmented-all-text\n",
      "ECU-ArSarcasem-Augmented-Test-all-text\n",
      "COS-ArSarcasem-Augmented-Test-all-text\n",
      "JAC-ArSarcasem-Augmented-Test-all-text\n",
      "BLE-ArSarcasem-Augmented-Test-all-text\n",
      "ECU-ArSarcasem-Augmented-Test-new-text\n",
      "COS-ArSarcasem-Augmented-Test-new-text\n",
      "JAC-ArSarcasem-Augmented-Test-new-text\n",
      "BLE-ArSarcasem-Augmented-Test-new-text\n",
      "ECU-ArSarcasem-Not-Augmented-Test-all-text\n",
      "COS-ArSarcasem-Not-Augmented-Test-all-text\n",
      "JAC-ArSarcasem-Not-Augmented-Test-all-text\n",
      "BLE-ArSarcasem-Not-Augmented-Test-all-text\n",
      "ECU-ArSarcasem-Not-Augmented-Test-new-text\n",
      "COS-ArSarcasem-Not-Augmented-Test-new-text\n",
      "JAC-ArSarcasem-Not-Augmented-Test-new-text\n",
      "BLE-ArSarcasem-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584f679",
   "metadata": {},
   "source": [
    "### PR-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29ae17a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "datasetname         = 'ArSarcasem'\n",
    "model_name          = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_models_dir      = \"models\\\\\" + datasetname + \"\\\\\"\n",
    "all_results         = []\n",
    "positive_label_key  = 2\n",
    "\n",
    "# ** find_folders_with_prefix **\n",
    "def find_folders_with_prefix(directory, search_str):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(search_str):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# ** compute_roc_and_pr **    \n",
    "def compute_roc_and_pr (true_labesl, predicted_labels, pos_label):\n",
    "    precision, recall, _ = precision_recall_curve (true_labels, predicted_labels, pos_label=pos_label) \n",
    "    pr_auc               = auc                    (recall, precision)\n",
    "    fpr, tpr, _          = roc_curve              (true_labels, predicted_labels, pos_label=pos_label)\n",
    "    roc_auc              = auc                    (fpr, tpr)\n",
    "   \n",
    "    return {    'fpr'           : fpr            ,\n",
    "                'tpr'           : tpr            ,\n",
    "                'precision'     : precision      ,\n",
    "                'recall'        : recall         ,\n",
    "                'roc_auc'       : roc_auc        ,\n",
    "                'pr_auc'        : pr_auc\n",
    "           }\n",
    "\n",
    "# ** get_fold_num_from_str ** \n",
    "def get_fold_num_from_str (text):\n",
    "    tmp = -1    \n",
    "    if '_0_' in text:\n",
    "        tmp = 0\n",
    "    elif '_1_' in text:\n",
    "        tmp = 1\n",
    "    elif '_2_' in text:\n",
    "        tmp = 2\n",
    "    elif '_3_' in text:\n",
    "        tmp = 3\n",
    "    elif '_4_' in text:\n",
    "        tmp = 4\n",
    "    else:\n",
    "        return -1    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR&ROC: Dataset-ArSarcasem-Not-Augmented-all-text, fold-0\n",
      "PR&ROC: Dataset-ArSarcasem-Not-Augmented-all-text, fold-1\n",
      "PR&ROC: Dataset-ArSarcasem-Not-Augmented-all-text, fold-2\n",
      "PR&ROC: Dataset-ArSarcasem-Not-Augmented-all-text, fold-3\n",
      "PR&ROC: Dataset-ArSarcasem-Not-Augmented-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ArSarcasem-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ArSarcasem-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ArSarcasem-Augmented-Test-all-text, fold-2\n"
     ]
    }
   ],
   "source": [
    "# process all datasets for finding PR and ROC    \n",
    "for dataset in all_datasets:       \n",
    "    selected_dataset                = copy.deepcopy(dataset)\n",
    "    dataset_name                    = selected_dataset.name\n",
    "    dataset_models_dirs             = []\n",
    "    arabic_prep                     = ArabertPreprocessor(model_name)\n",
    "    label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "    fold_num                        = 0\n",
    "    dataset_models_dirs             = find_folders_with_prefix(all_models_dir, dataset_name)\n",
    "    for directory in dataset_models_dirs:\n",
    "        fold_num     = get_fold_num_from_str(directory)\n",
    "        print(\"PR&ROC: Dataset-\" + dataset_name + \", fold-\" + str(fold_num))\n",
    "        model_dir    = all_models_dir + directory\n",
    "        config_dir   = all_models_dir + directory + \"\\\\config.json\"\n",
    "        model        = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        config       = AutoConfig.from_pretrained(config_dir)\n",
    "        tokenizer    = AutoTokenizer.from_pretrained(model_name)\n",
    "         \n",
    "        # Use DataLoader to process the data in batches\n",
    "        test_data    = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)).to_list()\n",
    "        test_labels  = selected_dataset.test[LABEL_COLUMN].to_list()\n",
    "        test_dataset = list(zip(test_data, test_labels))\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "         \n",
    "        predicted_probabilities = []\n",
    "        true_labels             = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                 #print(batch)\n",
    "                batch_data, label_data = batch #batch_data   = [data for data, _ in batch]\n",
    "                batch_labels           = [label_map[label] for label in label_data]\n",
    "                inputs                 = tokenizer(batch_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                outputs                = model(**inputs)\n",
    "                batch_probs            = outputs.logits.softmax(dim=-1).detach().numpy()\n",
    "                predicted_probabilities.append(batch_probs)\n",
    "                true_labels.extend(batch_labels)\n",
    "                \n",
    "        predicted_probabilities = np.concatenate(predicted_probabilities, axis=0)\n",
    "        predicted_labels        = predicted_probabilities.argmax(axis=-1)\n",
    "        results                 = compute_roc_and_pr(true_labels, predicted_labels, positive_label_key)\n",
    "        results['Dataset_Name'] = dataset_name\n",
    "        results['Fold_No']      = fold_num\n",
    "        all_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12117f38",
   "metadata": {},
   "source": [
    "### Export Results to Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0ca21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"LatestResults/ArSarcasem/ArSarcasem-PR-ROC.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
