{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f566b38",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307cfc6",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for ASTD dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752029f7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c6586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba95086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ce6168",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd38bc",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>!!.</td>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>0.014882844,-0.051557414,-0.028316082,0.014168...</td>\n",
       "      <td>0.01946623,-0.010952667,-0.039843258,-0.057320...</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['!!.']</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و #فسسسسسس</td>\n",
       "      <td>NEG</td>\n",
       "      <td>لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...</td>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017838204,0.007064947,-0.03709342,-0.0264731...</td>\n",
       "      <td>0.205765</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>.</td>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>0.026780926,0.009709039,-0.030822175,-0.033138...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.237325</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>.</td>\n",
       "      <td># السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...</td>\n",
       "      <td>0.011575715,-0.0191376,-0.041333534,-0.0137087...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...</td>\n",
       "      <td>['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>في الشوارع.</td>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017580768,-0.0027376027,-0.03825421,-0.04189...</td>\n",
       "      <td>0.390541</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['في', 'الشوارع.']</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...   NEG  \\\n",
       "1          دباسم يوسف عمل برنامج البرنامج و #فسسسسسس   NEG   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   NEG   \n",
       "3  #السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...   POS   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   NEG   \n",
       "\n",
       "                                            new_text   \n",
       "0                                                !!.  \\\n",
       "1  لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...   \n",
       "2                                                  .   \n",
       "3                                                  .   \n",
       "4                                        في الشوارع.   \n",
       "\n",
       "                                            all_text   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...  \\\n",
       "1  دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   \n",
       "3  # السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   \n",
       "\n",
       "                                 original_embbedding   \n",
       "0  0.014882844,-0.051557414,-0.028316082,0.014168...  \\\n",
       "1  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "2  0.026780926,0.009709039,-0.030822175,-0.033138...   \n",
       "3  0.011575715,-0.0191376,-0.041333534,-0.0137087...   \n",
       "4  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "\n",
       "                                      new_embbedding  ecu_similarity   \n",
       "0  0.01946623,-0.010952667,-0.039843258,-0.057320...        0.772223  \\\n",
       "1  0.017838204,0.007064947,-0.03709342,-0.0264731...        0.205765   \n",
       "2  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.237325   \n",
       "3  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.352307   \n",
       "4  0.017580768,-0.0027376027,-0.03825421,-0.04189...        0.390541   \n",
       "\n",
       "   cos_similarity  jacc_similarity   \n",
       "0           0.446         0.037037  \\\n",
       "1           0.929         0.437500   \n",
       "2           0.904         0.000000   \n",
       "3           0.829         0.000000   \n",
       "4           0.834         0.360000   \n",
       "\n",
       "                                          text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      all_text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      new_text_split  bleu_sim_1  bleu_sim_2   \n",
       "0                                            ['!!.']        0.89        0.89  \\\n",
       "1  ['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...        0.14        0.13   \n",
       "2                                              ['.']        0.95        0.95   \n",
       "3                                              ['.']        0.79        0.78   \n",
       "4                                 ['في', 'الشوارع.']        0.92        0.92   \n",
       "\n",
       "   bleu_sim_3  bleu_sim_4  \n",
       "0        0.88        0.88  \n",
       "1        0.12        0.11  \n",
       "2        0.95        0.95  \n",
       "3        0.78        0.77  \n",
       "4        0.92        0.92  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'ASTD'\n",
    "datasetpath = \"Augmented-Dataset/xls/ASTD-Unbalanced-Augmented-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017d22bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECU': 0.33158923031772564,\n",
       " 'COS': 0.8526818791946309,\n",
       " 'JAC': 0.3624915367325659,\n",
       " 'BLEU': 0.3949177274138466}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "all_datasets= []\n",
    "SIM_COFFICIENTS_THRESHOLDS = {'ECU': df[\"ecu_similarity\"].mean(), 'COS':df[\"cos_similarity\"].mean(), 'JAC':df[\"jacc_similarity\"].mean(), 'BLEU':df[\"bleu_sim_1\"].mean()}\n",
    "LABEL_TO_AUGMENT = ['NEG', 'NEUTRAL']\n",
    "DATA_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "SIM_COFFICIENTS_THRESHOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1190f",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dc34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3363",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14909da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83535fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b294d0",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0e8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58ba",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4ecc7",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32d16716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTD-Not-Augmented-all-text\n",
      "ECU-ASTD-Augmented-Test-all-text\n",
      "COS-ASTD-Augmented-Test-all-text\n",
      "JAC-ASTD-Augmented-Test-all-text\n",
      "BLE-ASTD-Augmented-Test-all-text\n",
      "ECU-ASTD-Augmented-Test-new-text\n",
      "COS-ASTD-Augmented-Test-new-text\n",
      "JAC-ASTD-Augmented-Test-new-text\n",
      "BLE-ASTD-Augmented-Test-new-text\n",
      "ECU-ASTD-Not-Augmented-Test-all-text\n",
      "COS-ASTD-Not-Augmented-Test-all-text\n",
      "JAC-ASTD-Not-Augmented-Test-all-text\n",
      "BLE-ASTD-Not-Augmented-Test-all-text\n",
      "ECU-ASTD-Not-Augmented-Test-new-text\n",
      "COS-ASTD-Not-Augmented-Test-new-text\n",
      "JAC-ASTD-Not-Augmented-Test-new-text\n",
      "BLE-ASTD-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05ff5e",
   "metadata": {},
   "source": [
    "### PR-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ed0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "datasetname         = 'ArSarcasem'\n",
    "model_name          = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_models_dir      = \"models\\\\\" + datasetname + \"\\\\\"\n",
    "all_results         = []\n",
    "positive_label_key  = 2\n",
    "\n",
    "# ** find_folders_with_prefix **\n",
    "def find_folders_with_prefix(directory, search_str):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(search_str):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# ** compute_roc_and_pr **    \n",
    "def compute_roc_and_pr (true_labesl, predicted_labels, pos_label):\n",
    "    precision, recall, _ = precision_recall_curve (true_labels, predicted_labels, pos_label=pos_label) \n",
    "    pr_auc               = auc                    (recall, precision)\n",
    "    fpr, tpr, _          = roc_curve              (true_labels, predicted_labels, pos_label=pos_label)\n",
    "    roc_auc              = auc                    (fpr, tpr)\n",
    "   \n",
    "    return {    'fpr'           : fpr            ,\n",
    "                'tpr'           : tpr            ,\n",
    "                'precision'     : precision      ,\n",
    "                'recall'        : recall         ,\n",
    "                'roc_auc'       : roc_auc        ,\n",
    "                'pr_auc'        : pr_auc\n",
    "           }\n",
    "\n",
    "# ** get_fold_num_from_str ** \n",
    "def get_fold_num_from_str (text):\n",
    "    tmp = -1    \n",
    "    if '_0_' in text:\n",
    "        tmp = 0\n",
    "    elif '_1_' in text:\n",
    "        tmp = 1\n",
    "    elif '_2_' in text:\n",
    "        tmp = 2\n",
    "    elif '_3_' in text:\n",
    "        tmp = 3\n",
    "    elif '_4_' in text:\n",
    "        tmp = 4\n",
    "    else:\n",
    "        return -1    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4254d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all datasets for finding PR and ROC    \n",
    "for dataset in all_datasets:       \n",
    "    selected_dataset                = copy.deepcopy(dataset)\n",
    "    dataset_name                    = selected_dataset.name\n",
    "    dataset_models_dirs             = []\n",
    "    arabic_prep                     = ArabertPreprocessor(model_name)\n",
    "    label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "    fold_num                        = 0\n",
    "    dataset_models_dirs             = find_folders_with_prefix(all_models_dir, dataset_name)\n",
    "    for directory in dataset_models_dirs:\n",
    "        fold_num     = get_fold_num_from_str(directory)\n",
    "        print(\"PR&ROC: Dataset-\" + dataset_name + \", fold-\" + str(fold_num))\n",
    "        model_dir    = all_models_dir + directory\n",
    "        config_dir   = all_models_dir + directory + \"\\\\config.json\"\n",
    "        model        = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        config       = AutoConfig.from_pretrained(config_dir)\n",
    "        tokenizer    = AutoTokenizer.from_pretrained(model_name)\n",
    "         \n",
    "        # Use DataLoader to process the data in batches\n",
    "        test_data    = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)).to_list()\n",
    "        test_labels  = selected_dataset.test[LABEL_COLUMN].to_list()\n",
    "        test_dataset = list(zip(test_data, test_labels))\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "         \n",
    "        predicted_probabilities = []\n",
    "        true_labels             = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                 #print(batch)\n",
    "                batch_data, label_data = batch #batch_data   = [data for data, _ in batch]\n",
    "                batch_labels           = [label_map[label] for label in label_data]\n",
    "                inputs                 = tokenizer(batch_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                outputs                = model(**inputs)\n",
    "                batch_probs            = outputs.logits.softmax(dim=-1).detach().numpy()\n",
    "                predicted_probabilities.append(batch_probs)\n",
    "                true_labels.extend(batch_labels)\n",
    "                \n",
    "        predicted_probabilities = np.concatenate(predicted_probabilities, axis=0)\n",
    "        predicted_labels        = predicted_probabilities.argmax(axis=-1)\n",
    "        results                 = compute_roc_and_pr(true_labels, predicted_labels, positive_label_key)\n",
    "        results['Dataset_Name'] = dataset_name\n",
    "        results['Fold_No']      = fold_num\n",
    "        all_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9af33",
   "metadata": {},
   "source": [
    "### Export Results to Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bb0ca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Fold_No</th>\n",
       "      <th>ci_macro_f1</th>\n",
       "      <th>ci_accuracy</th>\n",
       "      <th>ci_precision</th>\n",
       "      <th>ci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.871929</td>\n",
       "      <td>0.449855</td>\n",
       "      <td>0.606589</td>\n",
       "      <td>0.415667</td>\n",
       "      <td>0.506268</td>\n",
       "      <td>78.5664</td>\n",
       "      <td>6.568</td>\n",
       "      <td>0.064</td>\n",
       "      <td>1.94</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.041740</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>0.042718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861870</td>\n",
       "      <td>0.468384</td>\n",
       "      <td>0.615534</td>\n",
       "      <td>0.550240</td>\n",
       "      <td>0.522938</td>\n",
       "      <td>88.0507</td>\n",
       "      <td>5.849</td>\n",
       "      <td>0.057</td>\n",
       "      <td>1.94</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>0.041566</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>0.042677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.859089</td>\n",
       "      <td>0.469846</td>\n",
       "      <td>0.607767</td>\n",
       "      <td>0.582988</td>\n",
       "      <td>0.498958</td>\n",
       "      <td>85.8649</td>\n",
       "      <td>5.998</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.94</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.042129</td>\n",
       "      <td>0.042722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.856436</td>\n",
       "      <td>0.455433</td>\n",
       "      <td>0.609709</td>\n",
       "      <td>0.568364</td>\n",
       "      <td>0.496458</td>\n",
       "      <td>86.0686</td>\n",
       "      <td>5.984</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.94</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>3</td>\n",
       "      <td>0.042552</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.042721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.855223</td>\n",
       "      <td>0.467208</td>\n",
       "      <td>0.625243</td>\n",
       "      <td>0.432347</td>\n",
       "      <td>0.523260</td>\n",
       "      <td>86.2387</td>\n",
       "      <td>5.972</td>\n",
       "      <td>0.058</td>\n",
       "      <td>1.94</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>4</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>0.042676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_macro_f1  eval_accuracy  eval_precision  eval_recall   \n",
       "0   0.871929       0.449855       0.606589        0.415667     0.506268  \\\n",
       "1   0.861870       0.468384       0.615534        0.550240     0.522938   \n",
       "2   0.859089       0.469846       0.607767        0.582988     0.498958   \n",
       "3   0.856436       0.455433       0.609709        0.568364     0.496458   \n",
       "4   0.855223       0.467208       0.625243        0.432347     0.523260   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second  epoch   \n",
       "0       78.5664                    6.568                  0.064   1.94  \\\n",
       "1       88.0507                    5.849                  0.057   1.94   \n",
       "2       85.8649                    5.998                  0.058   1.94   \n",
       "3       86.0686                    5.984                  0.058   1.94   \n",
       "4       86.2387                    5.972                  0.058   1.94   \n",
       "\n",
       "                  Dataset_Name  Fold_No  ci_macro_f1  ci_accuracy   \n",
       "0  ASTD-Not-Augmented-all-text        0     0.042506     0.041740  \\\n",
       "1  ASTD-Not-Augmented-all-text        1     0.042636     0.041566   \n",
       "2  ASTD-Not-Augmented-all-text        2     0.042644     0.041718   \n",
       "3  ASTD-Not-Augmented-all-text        3     0.042552     0.041681   \n",
       "4  ASTD-Not-Augmented-all-text        4     0.042630     0.041360   \n",
       "\n",
       "   ci_precision  ci_recall  \n",
       "0      0.042110   0.042718  \n",
       "1      0.042506   0.042677  \n",
       "2      0.042129   0.042722  \n",
       "3      0.042321   0.042721  \n",
       "4      0.042329   0.042676  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "761f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainResults.to_excel(\"LatestResults/ASTD/ASTD-Results-1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a40f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e85c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c499d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f7f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
