{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4997f0ab",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cd11f",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for MOV dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c1c013",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309e7684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be04ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd840480",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72608a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab6f7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe32db",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7aa540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عظماء ولكن في أشياء كتير أوي ممكن تخلي أي فيلم...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>عظماء ولكن في أشياء كتير أوي ممكن تخلي أي فيلم...</td>\n",
       "      <td>0.023597183,0.01086875,-0.03102856,-0.03223265...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['عظماء', 'ولكن', 'في', 'أشياء', 'كتير', 'أوي'...</td>\n",
       "      <td>['عظماء', 'ولكن', 'في', 'أشياء', 'كتير', 'أوي'...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>حقا انه افضل افلام السينما المصرية فى البداية ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>حقا انه افضل افلام السينما المصرية فى البداية ...</td>\n",
       "      <td>0.030677188,0.040905435,-0.031328138,-0.030169...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['حقا', 'انه', 'افضل', 'افلام', 'السينما', 'ال...</td>\n",
       "      <td>['حقا', 'انه', 'افضل', 'افلام', 'السينما', 'ال...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>كبت وحرمان وغيرة بعيدا عن تصوير حالات وقضايا ا...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>كبت وحرمان وغيرة بعيدا عن تصوير حالات وقضايا ا...</td>\n",
       "      <td>0.014913844,0.027157936,-0.030840687,-0.023250...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['كبت', 'وحرمان', 'وغيرة', 'بعيدا', 'عن', 'تصو...</td>\n",
       "      <td>['كبت', 'وحرمان', 'وغيرة', 'بعيدا', 'عن', 'تصو...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>المومياء...يوم ان يُحصى الابداع! هناك فى مجال ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>المومياء . . يوم ان يحصى الابداع ! هناك فى مجا...</td>\n",
       "      <td>0.036414325,0.028412146,-0.037543822,-0.037372...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['المومياء...يوم', 'ان', 'يُحصى', 'الابداع!', ...</td>\n",
       "      <td>['المومياء', '.', '.', 'يوم', 'ان', 'يحصى', 'ا...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مفيش جديد للاسف فى اوائل حلقات مسلسل الصفعة اش...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>مفيش جديد للاسف فى اوائل حلقات مسلسل الصفعة اش...</td>\n",
       "      <td>0.026780926,0.009709039,-0.030822175,-0.033138...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['مفيش', 'جديد', 'للاسف', 'فى', 'اوائل', 'حلقا...</td>\n",
       "      <td>['مفيش', 'جديد', 'للاسف', 'فى', 'اوائل', 'حلقا...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label new_text   \n",
       "0  عظماء ولكن في أشياء كتير أوي ممكن تخلي أي فيلم...   NEG      NaN  \\\n",
       "1  حقا انه افضل افلام السينما المصرية فى البداية ...   POS      NaN   \n",
       "2  كبت وحرمان وغيرة بعيدا عن تصوير حالات وقضايا ا...   POS      NaN   \n",
       "3  المومياء...يوم ان يُحصى الابداع! هناك فى مجال ...   POS      NaN   \n",
       "4  مفيش جديد للاسف فى اوائل حلقات مسلسل الصفعة اش...   NEG      NaN   \n",
       "\n",
       "                                            all_text   \n",
       "0  عظماء ولكن في أشياء كتير أوي ممكن تخلي أي فيلم...  \\\n",
       "1  حقا انه افضل افلام السينما المصرية فى البداية ...   \n",
       "2  كبت وحرمان وغيرة بعيدا عن تصوير حالات وقضايا ا...   \n",
       "3  المومياء . . يوم ان يحصى الابداع ! هناك فى مجا...   \n",
       "4  مفيش جديد للاسف فى اوائل حلقات مسلسل الصفعة اش...   \n",
       "\n",
       "                                 original_embbedding new_embbedding   \n",
       "0  0.023597183,0.01086875,-0.03102856,-0.03223265...              0  \\\n",
       "1  0.030677188,0.040905435,-0.031328138,-0.030169...              0   \n",
       "2  0.014913844,0.027157936,-0.030840687,-0.023250...              0   \n",
       "3  0.036414325,0.028412146,-0.037543822,-0.037372...              0   \n",
       "4  0.026780926,0.009709039,-0.030822175,-0.033138...              0   \n",
       "\n",
       "   ecu_similarity  cos_similarity  jacc_similarity   \n",
       "0        0.023597             NaN              0.0  \\\n",
       "1        0.030677             NaN              0.0   \n",
       "2        0.014914             NaN              0.0   \n",
       "3        0.036414             NaN              0.0   \n",
       "4        0.026781             NaN              0.0   \n",
       "\n",
       "                                          text_split   \n",
       "0  ['عظماء', 'ولكن', 'في', 'أشياء', 'كتير', 'أوي'...  \\\n",
       "1  ['حقا', 'انه', 'افضل', 'افلام', 'السينما', 'ال...   \n",
       "2  ['كبت', 'وحرمان', 'وغيرة', 'بعيدا', 'عن', 'تصو...   \n",
       "3  ['المومياء...يوم', 'ان', 'يُحصى', 'الابداع!', ...   \n",
       "4  ['مفيش', 'جديد', 'للاسف', 'فى', 'اوائل', 'حلقا...   \n",
       "\n",
       "                                      all_text_split new_text_split   \n",
       "0  ['عظماء', 'ولكن', 'في', 'أشياء', 'كتير', 'أوي'...        ['nan']  \\\n",
       "1  ['حقا', 'انه', 'افضل', 'افلام', 'السينما', 'ال...        ['nan']   \n",
       "2  ['كبت', 'وحرمان', 'وغيرة', 'بعيدا', 'عن', 'تصو...        ['nan']   \n",
       "3  ['المومياء', '.', '.', 'يوم', 'ان', 'يحصى', 'ا...        ['nan']   \n",
       "4  ['مفيش', 'جديد', 'للاسف', 'فى', 'اوائل', 'حلقا...        ['nan']   \n",
       "\n",
       "   bleu_sim_1  bleu_sim_2  bleu_sim_3  bleu_sim_4  \n",
       "0        0.02        0.01        0.01        0.01  \n",
       "1        0.25        0.23        0.22        0.22  \n",
       "2        0.00        0.00        0.00        0.00  \n",
       "3        0.00        0.00        0.00        0.00  \n",
       "4        0.12        0.11        0.11        0.11  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'MOV'\n",
    "datasetpath = \"Augmented-Dataset/xls/MOV-Augmented-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324ec45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN  = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "all_datasets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275c573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cb074",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5f1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beaae93",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e4b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36ab0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8123517",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c76aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9851a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df18909",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "112729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48607c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03082759",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c3a9110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOV-Not-Augmented-all-text\n",
      "ECU-MOV-Augmented-Test-all-text\n",
      "COS-MOV-Augmented-Test-all-text\n",
      "JAC-MOV-Augmented-Test-all-text\n",
      "BLE-MOV-Augmented-Test-all-text\n",
      "ECU-MOV-Augmented-Test-new-text\n",
      "COS-MOV-Augmented-Test-new-text\n",
      "JAC-MOV-Augmented-Test-new-text\n",
      "BLE-MOV-Augmented-Test-new-text\n",
      "ECU-MOV-Not-Augmented-Test-all-text\n",
      "COS-MOV-Not-Augmented-Test-all-text\n",
      "JAC-MOV-Not-Augmented-Test-all-text\n",
      "BLE-MOV-Not-Augmented-Test-all-text\n",
      "ECU-MOV-Not-Augmented-Test-new-text\n",
      "COS-MOV-Not-Augmented-Test-new-text\n",
      "JAC-MOV-Not-Augmented-Test-new-text\n",
      "BLE-MOV-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09a63e",
   "metadata": {},
   "source": [
    "### PR-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c602199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "datasetname         = 'MOV'\n",
    "model_name          = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_models_dir      = \"models\\\\\" + datasetname + \"\\\\\"\n",
    "all_results         = []\n",
    "positive_label_key  = 1 # {'NEG': 0, 'POS': 1, 'NEUTRAL': 2}\n",
    "\n",
    "# ** find_folders_with_prefix **\n",
    "def find_folders_with_prefix(directory, search_str):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(search_str):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# ** compute_roc_and_pr **    \n",
    "def compute_roc_and_pr (true_labesl, predicted_labels, pos_label):\n",
    "    precision, recall, _ = precision_recall_curve (true_labels, predicted_labels, pos_label=pos_label) \n",
    "    pr_auc               = auc                    (recall, precision)\n",
    "    fpr, tpr, _          = roc_curve              (true_labels, predicted_labels, pos_label=pos_label)\n",
    "    roc_auc              = auc                    (fpr, tpr)\n",
    "   \n",
    "    return {    'fpr'           : fpr            ,\n",
    "                'tpr'           : tpr            ,\n",
    "                'precision'     : precision      ,\n",
    "                'recall'        : recall         ,\n",
    "                'roc_auc'       : roc_auc        ,\n",
    "                'pr_auc'        : pr_auc\n",
    "           }\n",
    "\n",
    "# ** get_fold_num_from_str ** \n",
    "def get_fold_num_from_str (text):\n",
    "    tmp = -1    \n",
    "    if '_0_' in text:\n",
    "        tmp = 0\n",
    "    elif '_1_' in text:\n",
    "        tmp = 1\n",
    "    elif '_2_' in text:\n",
    "        tmp = 2\n",
    "    elif '_3_' in text:\n",
    "        tmp = 3\n",
    "    elif '_4_' in text:\n",
    "        tmp = 4\n",
    "    else:\n",
    "        return -1    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c2d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NEG': 0, 'POS': 1, 'NEUTRAL': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_dataset                = copy.deepcopy(all_datasets[0])\n",
    "label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b5a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all datasets for finding PR and ROC    \n",
    "# ['NEG', 'POS', 'NEUTRAL'] ******************************************\n",
    "for dataset in all_datasets:       \n",
    "    selected_dataset                = copy.deepcopy(dataset)\n",
    "    dataset_name                    = selected_dataset.name\n",
    "    dataset_models_dirs             = []\n",
    "    arabic_prep                     = ArabertPreprocessor(model_name)\n",
    "    label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "    fold_num                        = 0\n",
    "    dataset_models_dirs             = find_folders_with_prefix(all_models_dir, dataset_name)\n",
    "    for directory in dataset_models_dirs:\n",
    "        fold_num     = get_fold_num_from_str(directory)\n",
    "        print(\"PR&ROC: Dataset-\" + dataset_name + \", fold-\" + str(fold_num))\n",
    "        model_dir    = all_models_dir + directory\n",
    "        config_dir   = all_models_dir + directory + \"\\\\config.json\"\n",
    "        model        = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        config       = AutoConfig.from_pretrained(config_dir)\n",
    "        tokenizer    = AutoTokenizer.from_pretrained(model_name)\n",
    "         \n",
    "        # Use DataLoader to process the data in batches\n",
    "        test_data    = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)).to_list()\n",
    "        test_labels  = selected_dataset.test[LABEL_COLUMN].to_list()\n",
    "        test_dataset = list(zip(test_data, test_labels))\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "         \n",
    "        predicted_probabilities = []\n",
    "        true_labels             = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                 #print(batch)\n",
    "                batch_data, label_data = batch #batch_data   = [data for data, _ in batch]\n",
    "                batch_labels           = [label_map[label] for label in label_data]\n",
    "                inputs                 = tokenizer(batch_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                outputs                = model(**inputs)\n",
    "                batch_probs            = outputs.logits.softmax(dim=-1).detach().numpy()\n",
    "                predicted_probabilities.append(batch_probs)\n",
    "                true_labels.extend(batch_labels)\n",
    "                \n",
    "        predicted_probabilities = np.concatenate(predicted_probabilities, axis=0)\n",
    "        predicted_labels        = predicted_probabilities.argmax(axis=-1)\n",
    "        results                 = compute_roc_and_pr(true_labels, predicted_labels, positive_label_key)\n",
    "        results['Dataset_Name'] = dataset_name\n",
    "        results['Fold_No']      = fold_num\n",
    "        all_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75764d0",
   "metadata": {},
   "source": [
    "### Export Results to Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1558fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12c836ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_macro_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Fold_No</th>\n",
       "      <th>ci_macro_f1</th>\n",
       "      <th>ci_accuracy</th>\n",
       "      <th>ci_precision</th>\n",
       "      <th>ci_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848714</td>\n",
       "      <td>0.270012</td>\n",
       "      <td>0.637860</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.338798</td>\n",
       "      <td>18.4957</td>\n",
       "      <td>13.138</td>\n",
       "      <td>0.108</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MOV-Not-Augmented-all-text</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055255</td>\n",
       "      <td>0.059817</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.058906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.855623</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.633745</td>\n",
       "      <td>0.378285</td>\n",
       "      <td>0.336633</td>\n",
       "      <td>29.8557</td>\n",
       "      <td>8.139</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MOV-Not-Augmented-all-text</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055177</td>\n",
       "      <td>0.059961</td>\n",
       "      <td>0.060357</td>\n",
       "      <td>0.058814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.864908</td>\n",
       "      <td>0.265203</td>\n",
       "      <td>0.621399</td>\n",
       "      <td>0.292538</td>\n",
       "      <td>0.332174</td>\n",
       "      <td>30.1550</td>\n",
       "      <td>8.058</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MOV-Not-Augmented-all-text</td>\n",
       "      <td>2</td>\n",
       "      <td>0.054941</td>\n",
       "      <td>0.060367</td>\n",
       "      <td>0.056619</td>\n",
       "      <td>0.058619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.835301</td>\n",
       "      <td>0.258228</td>\n",
       "      <td>0.632231</td>\n",
       "      <td>0.210744</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>30.4332</td>\n",
       "      <td>7.952</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MOV-Not-Augmented-all-text</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054470</td>\n",
       "      <td>0.060013</td>\n",
       "      <td>0.050759</td>\n",
       "      <td>0.058670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.858389</td>\n",
       "      <td>0.290973</td>\n",
       "      <td>0.644628</td>\n",
       "      <td>0.546722</td>\n",
       "      <td>0.349462</td>\n",
       "      <td>29.9040</td>\n",
       "      <td>8.093</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.0</td>\n",
       "      <td>MOV-Not-Augmented-all-text</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056530</td>\n",
       "      <td>0.059569</td>\n",
       "      <td>0.061957</td>\n",
       "      <td>0.059342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_macro_f1  eval_accuracy  eval_precision  eval_recall   \n",
       "0   0.848714       0.270012       0.637860        0.545455     0.338798  \\\n",
       "1   0.855623       0.268810       0.633745        0.378285     0.336633   \n",
       "2   0.864908       0.265203       0.621399        0.292538     0.332174   \n",
       "3   0.835301       0.258228       0.632231        0.210744     0.333333   \n",
       "4   0.858389       0.290973       0.644628        0.546722     0.349462   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second  epoch   \n",
       "0       18.4957                   13.138                  0.108    2.0  \\\n",
       "1       29.8557                    8.139                  0.067    2.0   \n",
       "2       30.1550                    8.058                  0.066    2.0   \n",
       "3       30.4332                    7.952                  0.066    2.0   \n",
       "4       29.9040                    8.093                  0.067    2.0   \n",
       "\n",
       "                 Dataset_Name  Fold_No  ci_macro_f1  ci_accuracy   \n",
       "0  MOV-Not-Augmented-all-text        0     0.055255     0.059817  \\\n",
       "1  MOV-Not-Augmented-all-text        1     0.055177     0.059961   \n",
       "2  MOV-Not-Augmented-all-text        2     0.054941     0.060367   \n",
       "3  MOV-Not-Augmented-all-text        3     0.054470     0.060013   \n",
       "4  MOV-Not-Augmented-all-text        4     0.056530     0.059569   \n",
       "\n",
       "   ci_precision  ci_recall  \n",
       "0      0.061971   0.058906  \n",
       "1      0.060357   0.058814  \n",
       "2      0.056619   0.058619  \n",
       "3      0.050759   0.058670  \n",
       "4      0.061957   0.059342  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8471aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"LatestResults/MOV/MOV-PR-ROC.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594750ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
