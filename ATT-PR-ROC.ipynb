{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d819f9",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84883cc",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for ATT dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3d4f22",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c6586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba95086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6e52c",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c029cee",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d7ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amal _x000D_\\nممتاز كبير محلات وكافيهات ومطاعم...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amal ممتاز كبير محلات وكافيهات ومطاعم ورخيص وا...</td>\n",
       "      <td>0.034628928,0.019831989,-0.046533976,-0.024621...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['Amal', '_x000D_', 'ممتاز', 'كبير', 'محلات', ...</td>\n",
       "      <td>['Amal', 'ممتاز', 'كبير', 'محلات', 'وكافيهات',...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>افضل مكان للإقامة _x000D_\\nخلال إقامتي في اسطن...</td>\n",
       "      <td>POS</td>\n",
       "      <td>...تأسست \" مجموعة فاين هوم العالمية \" في عام 2...</td>\n",
       "      <td>افضل مكان للإقامة خلال إقامتي في اسطنبول أقمت ...</td>\n",
       "      <td>0.036414325,0.028412146,-0.037543822,-0.037372...</td>\n",
       "      <td>0.022658791,-0.003618797,-0.033782683,-0.04375...</td>\n",
       "      <td>0.345040</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>['افضل', 'مكان', 'للإقامة', '_x000D_', 'خلال',...</td>\n",
       "      <td>['افضل', 'مكان', 'للإقامة', 'خلال', 'إقامتي', ...</td>\n",
       "      <td>['...تأسست', '\"', 'مجموعة', 'فاين', 'هوم', 'ال...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>التلفريك في البرازيل _x000D_\\nأنصح لمن يزور ال...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>التلفريك في البرازيل أنصح لمن يزور البرازيل ان...</td>\n",
       "      <td>0.036414325,0.028412146,-0.037543822,-0.037372...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['التلفريك', 'في', 'البرازيل', '_x000D_', 'أنص...</td>\n",
       "      <td>['التلفريك', 'في', 'البرازيل', 'أنصح', 'لمن', ...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>رحله بحريه رائعه الى جزيره تيران _x000D_\\nالاس...</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>رحله بحريه رائعه الى جزيره تيران الاسماك والشع...</td>\n",
       "      <td>0.025511466,0.027461855,-0.022016473,-0.029629...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025511</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['رحله', 'بحريه', 'رائعه', 'الى', 'جزيره', 'تي...</td>\n",
       "      <td>['رحله', 'بحريه', 'رائعه', 'الى', 'جزيره', 'تي...</td>\n",
       "      <td>['nan']</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>جميلة _x000D_\\nالمكان جميل و التماثيل رائعة و ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>جدا.لوحة تحكم العضو الرسائل الخاصة الاشتراكات...</td>\n",
       "      <td>جميلة المكان جميل و التماثيل رائعة و انيميشن ظ...</td>\n",
       "      <td>0.024910163,0.041109513,-0.031470675,-0.017475...</td>\n",
       "      <td>0.017580774,-0.0027376045,-0.03825421,-0.04189...</td>\n",
       "      <td>0.450412</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>['جميلة', '_x000D_', 'المكان', 'جميل', 'و', 'ا...</td>\n",
       "      <td>['جميلة', 'المكان', 'جميل', 'و', 'التماثيل', '...</td>\n",
       "      <td>['جدا.لوحة', 'تحكم', 'العضو', 'الرسائل', 'الخا...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label   \n",
       "0  Amal _x000D_\\nممتاز كبير محلات وكافيهات ومطاعم...   POS  \\\n",
       "1  افضل مكان للإقامة _x000D_\\nخلال إقامتي في اسطن...   POS   \n",
       "2  التلفريك في البرازيل _x000D_\\nأنصح لمن يزور ال...   POS   \n",
       "3  رحله بحريه رائعه الى جزيره تيران _x000D_\\nالاس...   POS   \n",
       "4  جميلة _x000D_\\nالمكان جميل و التماثيل رائعة و ...   POS   \n",
       "\n",
       "                                            new_text   \n",
       "0                                                NaN  \\\n",
       "1  ...تأسست \" مجموعة فاين هوم العالمية \" في عام 2...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4   جدا.لوحة تحكم العضو الرسائل الخاصة الاشتراكات...   \n",
       "\n",
       "                                            all_text   \n",
       "0  Amal ممتاز كبير محلات وكافيهات ومطاعم ورخيص وا...  \\\n",
       "1  افضل مكان للإقامة خلال إقامتي في اسطنبول أقمت ...   \n",
       "2  التلفريك في البرازيل أنصح لمن يزور البرازيل ان...   \n",
       "3  رحله بحريه رائعه الى جزيره تيران الاسماك والشع...   \n",
       "4  جميلة المكان جميل و التماثيل رائعة و انيميشن ظ...   \n",
       "\n",
       "                                 original_embbedding   \n",
       "0  0.034628928,0.019831989,-0.046533976,-0.024621...  \\\n",
       "1  0.036414325,0.028412146,-0.037543822,-0.037372...   \n",
       "2  0.036414325,0.028412146,-0.037543822,-0.037372...   \n",
       "3  0.025511466,0.027461855,-0.022016473,-0.029629...   \n",
       "4  0.024910163,0.041109513,-0.031470675,-0.017475...   \n",
       "\n",
       "                                      new_embbedding  ecu_similarity   \n",
       "0                                                  0        0.034629  \\\n",
       "1  0.022658791,-0.003618797,-0.033782683,-0.04375...        0.345040   \n",
       "2                                                  0        0.036414   \n",
       "3                                                  0        0.025511   \n",
       "4  0.017580774,-0.0027376045,-0.03825421,-0.04189...        0.450412   \n",
       "\n",
       "   cos_similarity  jacc_similarity   \n",
       "0             NaN         0.000000  \\\n",
       "1           0.892         0.484848   \n",
       "2             NaN         0.000000   \n",
       "3             NaN         0.000000   \n",
       "4           0.790         0.633333   \n",
       "\n",
       "                                          text_split   \n",
       "0  ['Amal', '_x000D_', 'ممتاز', 'كبير', 'محلات', ...  \\\n",
       "1  ['افضل', 'مكان', 'للإقامة', '_x000D_', 'خلال',...   \n",
       "2  ['التلفريك', 'في', 'البرازيل', '_x000D_', 'أنص...   \n",
       "3  ['رحله', 'بحريه', 'رائعه', 'الى', 'جزيره', 'تي...   \n",
       "4  ['جميلة', '_x000D_', 'المكان', 'جميل', 'و', 'ا...   \n",
       "\n",
       "                                      all_text_split   \n",
       "0  ['Amal', 'ممتاز', 'كبير', 'محلات', 'وكافيهات',...  \\\n",
       "1  ['افضل', 'مكان', 'للإقامة', 'خلال', 'إقامتي', ...   \n",
       "2  ['التلفريك', 'في', 'البرازيل', 'أنصح', 'لمن', ...   \n",
       "3  ['رحله', 'بحريه', 'رائعه', 'الى', 'جزيره', 'تي...   \n",
       "4  ['جميلة', 'المكان', 'جميل', 'و', 'التماثيل', '...   \n",
       "\n",
       "                                      new_text_split  bleu_sim_1  bleu_sim_2   \n",
       "0                                            ['nan']        0.78        0.76  \\\n",
       "1  ['...تأسست', '\"', 'مجموعة', 'فاين', 'هوم', 'ال...        0.21        0.20   \n",
       "2                                            ['nan']        0.87        0.85   \n",
       "3                                            ['nan']        0.24        0.23   \n",
       "4  ['جدا.لوحة', 'تحكم', 'العضو', 'الرسائل', 'الخا...        0.13        0.12   \n",
       "\n",
       "   bleu_sim_3  bleu_sim_4  \n",
       "0        0.74        0.72  \n",
       "1        0.19        0.18  \n",
       "2        0.84        0.82  \n",
       "3        0.22        0.21  \n",
       "4        0.12        0.12  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'ATT'\n",
    "datasetpath = \"Augmented-Dataset/xls/ATT-Augmented-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "017d22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN  = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "all_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1190f",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "051195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3363",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14909da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83535fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b294d0",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58ba",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a02627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83a9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6344ad7",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32d16716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATT-Not-Augmented-all-text\n",
      "ECU-ATT-Augmented-Test-all-text\n",
      "COS-ATT-Augmented-Test-all-text\n",
      "JAC-ATT-Augmented-Test-all-text\n",
      "BLE-ATT-Augmented-Test-all-text\n",
      "ECU-ATT-Augmented-Test-new-text\n",
      "COS-ATT-Augmented-Test-new-text\n",
      "JAC-ATT-Augmented-Test-new-text\n",
      "BLE-ATT-Augmented-Test-new-text\n",
      "ECU-ATT-Not-Augmented-Test-all-text\n",
      "COS-ATT-Not-Augmented-Test-all-text\n",
      "JAC-ATT-Not-Augmented-Test-all-text\n",
      "BLE-ATT-Not-Augmented-Test-all-text\n",
      "ECU-ATT-Not-Augmented-Test-new-text\n",
      "COS-ATT-Not-Augmented-Test-new-text\n",
      "JAC-ATT-Not-Augmented-Test-new-text\n",
      "BLE-ATT-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54745d",
   "metadata": {},
   "source": [
    "### PR-ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d1329c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "datasetname         = 'ATT'\n",
    "model_name          = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_models_dir      = \"models\\\\\" + datasetname + \"\\\\\"\n",
    "all_results         = []\n",
    "positive_label_key  = 0 # {'POS': 0, 'NEG': 1}\n",
    "\n",
    "# ** find_folders_with_prefix **\n",
    "def find_folders_with_prefix(directory, search_str):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(search_str):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# ** compute_roc_and_pr **    \n",
    "def compute_roc_and_pr (true_labesl, predicted_labels, pos_label):\n",
    "    precision, recall, _ = precision_recall_curve (true_labels, predicted_labels, pos_label=pos_label) \n",
    "    pr_auc               = auc                    (recall, precision)\n",
    "    fpr, tpr, _          = roc_curve              (true_labels, predicted_labels, pos_label=pos_label)\n",
    "    roc_auc              = auc                    (fpr, tpr)\n",
    "   \n",
    "    return {    'fpr'           : fpr            ,\n",
    "                'tpr'           : tpr            ,\n",
    "                'precision'     : precision      ,\n",
    "                'recall'        : recall         ,\n",
    "                'roc_auc'       : roc_auc        ,\n",
    "                'pr_auc'        : pr_auc\n",
    "           }\n",
    "\n",
    "# ** get_fold_num_from_str ** \n",
    "def get_fold_num_from_str (text):\n",
    "    tmp = -1    \n",
    "    if '_0_' in text:\n",
    "        tmp = 0\n",
    "    elif '_1_' in text:\n",
    "        tmp = 1\n",
    "    elif '_2_' in text:\n",
    "        tmp = 2\n",
    "    elif '_3_' in text:\n",
    "        tmp = 3\n",
    "    elif '_4_' in text:\n",
    "        tmp = 4\n",
    "    else:\n",
    "        return -1    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0cbfb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR&ROC: Dataset-ATT-Not-Augmented-all-text, fold-0\n",
      "PR&ROC: Dataset-ATT-Not-Augmented-all-text, fold-1\n",
      "PR&ROC: Dataset-ATT-Not-Augmented-all-text, fold-2\n",
      "PR&ROC: Dataset-ATT-Not-Augmented-all-text, fold-3\n",
      "PR&ROC: Dataset-ATT-Not-Augmented-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ATT-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-COS-ATT-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ATT-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ATT-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ATT-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-COS-ATT-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ATT-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ATT-Not-Augmented-Test-new-text, fold-4\n"
     ]
    }
   ],
   "source": [
    "# process all datasets for finding PR and ROC    \n",
    "# ['NEG', 'POS', 'NEUTRAL'] ******************************************\n",
    "for dataset in all_datasets:       \n",
    "    selected_dataset                = copy.deepcopy(dataset)\n",
    "    dataset_name                    = selected_dataset.name\n",
    "    dataset_models_dirs             = []\n",
    "    arabic_prep                     = ArabertPreprocessor(model_name)\n",
    "    label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "    fold_num                        = 0\n",
    "    dataset_models_dirs             = find_folders_with_prefix(all_models_dir, dataset_name)\n",
    "    for directory in dataset_models_dirs:\n",
    "        fold_num     = get_fold_num_from_str(directory)\n",
    "        print(\"PR&ROC: Dataset-\" + dataset_name + \", fold-\" + str(fold_num))\n",
    "        model_dir    = all_models_dir + directory\n",
    "        config_dir   = all_models_dir + directory + \"\\\\config.json\"\n",
    "        model        = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        config       = AutoConfig.from_pretrained(config_dir)\n",
    "        tokenizer    = AutoTokenizer.from_pretrained(model_name)\n",
    "         \n",
    "        # Use DataLoader to process the data in batches\n",
    "        test_data    = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)).to_list()\n",
    "        test_labels  = selected_dataset.test[LABEL_COLUMN].to_list()\n",
    "        test_dataset = list(zip(test_data, test_labels))\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "         \n",
    "        predicted_probabilities = []\n",
    "        true_labels             = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                 #print(batch)\n",
    "                batch_data, label_data = batch #batch_data   = [data for data, _ in batch]\n",
    "                batch_labels           = [label_map[label] for label in label_data]\n",
    "                inputs                 = tokenizer(batch_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                outputs                = model(**inputs)\n",
    "                batch_probs            = outputs.logits.softmax(dim=-1).detach().numpy()\n",
    "                predicted_probabilities.append(batch_probs)\n",
    "                true_labels.extend(batch_labels)\n",
    "                \n",
    "        predicted_probabilities = np.concatenate(predicted_probabilities, axis=0)\n",
    "        predicted_labels        = predicted_probabilities.argmax(axis=-1)\n",
    "        results                 = compute_roc_and_pr(true_labels, predicted_labels, positive_label_key)\n",
    "        results['Dataset_Name'] = dataset_name\n",
    "        results['Fold_No']      = fold_num\n",
    "        all_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891fe877",
   "metadata": {},
   "source": [
    "### Export Results to Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "369ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bb0ca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Fold_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.9675174013921114, 1.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>ATT-Not-Augmented-all-text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.9675174013921114, 1.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>ATT-Not-Augmented-all-text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.9675174013921114, 1.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>ATT-Not-Augmented-all-text</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.9675174013921114, 1.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>ATT-Not-Augmented-all-text</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>[0.9675174013921114, 1.0]</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.983759</td>\n",
       "      <td>ATT-Not-Augmented-all-text</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fpr         tpr                  precision      recall  roc_auc   \n",
       "0  [0.0, 1.0]  [0.0, 1.0]  [0.9675174013921114, 1.0]  [1.0, 0.0]      0.5  \\\n",
       "1  [0.0, 1.0]  [0.0, 1.0]  [0.9675174013921114, 1.0]  [1.0, 0.0]      0.5   \n",
       "2  [0.0, 1.0]  [0.0, 1.0]  [0.9675174013921114, 1.0]  [1.0, 0.0]      0.5   \n",
       "3  [0.0, 1.0]  [0.0, 1.0]  [0.9675174013921114, 1.0]  [1.0, 0.0]      0.5   \n",
       "4  [0.0, 1.0]  [0.0, 1.0]  [0.9675174013921114, 1.0]  [1.0, 0.0]      0.5   \n",
       "\n",
       "     pr_auc                Dataset_Name  Fold_No  \n",
       "0  0.983759  ATT-Not-Augmented-all-text        0  \n",
       "1  0.983759  ATT-Not-Augmented-all-text        1  \n",
       "2  0.983759  ATT-Not-Augmented-all-text        2  \n",
       "3  0.983759  ATT-Not-Augmented-all-text        3  \n",
       "4  0.983759  ATT-Not-Augmented-all-text        4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "761f6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"LatestResults/ATT/ATT-PR-ROC.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a40f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
