{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f566b38",
   "metadata": {},
   "source": [
    " \n",
    "<img width=\"200px\" height=\"200px\" src='logo-en.png'/>\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D\">King Abdullah I School of Graduate Studies and Scientific Research</div><br/>\n",
    "<div style=\"text-align: center; font-size:20px; font-weight:bold; color: #212F3D;\">Data Augmentation using Transformers and Similarity Measures for Improving Arabic Text Classification</div><br/>\n",
    "<div style=\"text-align: center; font-size:14px; font-weight:bold; color: #212F3D\">Dania Refai<sup>1</sup>, Saleh Abu-Soud<sup>2</sup>, Mohammad Abdel-Rahman<sup>3</sup></div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>1</sup> Department of Computer Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>2</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:normal; color: #212F3D\">\n",
    "    <sup>3</sup> Department of Data Science, Princess Sumaya University for Technology (PSUT), Amman, Jordan</div>\n",
    "<br/>\n",
    "\n",
    "<div style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">\n",
    "        Crosspending author: Dania Refai (<span style=\"text-align: left; font-size:16px; font-weight:bold; color: #6495ED\">Dania.Refai@hotmail.com</span>).\n",
    "</div>\n",
    "<br/>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307cfc6",
   "metadata": {},
   "source": [
    "### <span style=\"text-align: left; font-size:20px; font-weight:bold; color: #C70039\">General Notes and Directions</span> ###\n",
    "<hr/>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you have pytorch installed on your machine. Moreover, if you want more information please refer to <a href=\"https://pytorch.org/\">INSTALL PYTORCH</a> from their official website.</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure your installed python version is 3.8</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Make sure you are running the commands INSIDE source code directory (<span style=\"color: #C70039\">.\\Implementation\\</span>)</li>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Run the following commands in your command shell to create and activate a Virtualenv (<span style=\"color: #C70039\">Windows based systems</span>):</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> set PATH=C:\\Users\\(<span style=\"text-align: left; font-size:14px; font-weight:bold; color: #C70039\">-windows_user-</span>)\\AppData\\Local\\Programs\\Python\\Python38\\\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m pip install --upgrade pip\n",
    "    </li>   \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%python.exe %PATH%Scripts\\pip.exe install virtualenv \n",
    "    </li>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> %PATH%\\python.exe -m virtualenv venv \n",
    "    </li>\n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Activate the virtual environment: </li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\activate\n",
    "    </li>  \n",
    "> </ol>\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp; Install requirements:</li>\n",
    "> <ol>    \n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install python-dotenv\n",
    "    </li>\n",
    "> <li style=\"text-align: left; font-family:console; font-size:14px; font-weight:bold; color: #212F3D; list-style-type: none;\">\n",
    "       <span style=\"color: #C70039\">cmd&gt;</span> .\\venv\\Scripts\\pip3 install -r requirements.txt\n",
    "    </li>   \n",
    "> </ol>\n",
    "\n",
    "> <li style=\"text-align: left; font-size:14px; font-weight:bold; color: #212F3D\">&nbsp;Notebook Purpose: <span style=\"color: #C70039\">Sentiment Analysis for ASTD dataset using Model: </span>aubmindlab/bert-base-arabertv02-twitter</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752029f7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9c6586b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba95086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from preprocess import ArabertPreprocessor\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, precision_recall_curve\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "import more_itertools\n",
    "import GPUtil as GPU\n",
    "import gc; \n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba127b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ce6168",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90f9a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "        super(ClassificationDataset).__init__()\n",
    "        self.text = text\n",
    "        self.target = target\n",
    "        self.tokenizer_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "          )      \n",
    "        return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29e385b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tThis custom dataset class will help us hold our datasets in a structred manner.\t\n",
    "'''\n",
    "class CustomDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        train: List[pd.DataFrame],\n",
    "        test: List[pd.DataFrame],\n",
    "        label_list: List[str],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd38bc",
   "metadata": {},
   "source": [
    "### Loading Training Dataset (Already Augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d7ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>new_text</th>\n",
       "      <th>all_text</th>\n",
       "      <th>original_embbedding</th>\n",
       "      <th>new_embbedding</th>\n",
       "      <th>ecu_similarity</th>\n",
       "      <th>cos_similarity</th>\n",
       "      <th>jacc_similarity</th>\n",
       "      <th>text_split</th>\n",
       "      <th>all_text_split</th>\n",
       "      <th>new_text_split</th>\n",
       "      <th>bleu_sim_1</th>\n",
       "      <th>bleu_sim_2</th>\n",
       "      <th>bleu_sim_3</th>\n",
       "      <th>bleu_sim_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>!!.</td>\n",
       "      <td>5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...</td>\n",
       "      <td>0.014882844,-0.051557414,-0.028316082,0.014168...</td>\n",
       "      <td>0.01946623,-0.010952667,-0.039843258,-0.057320...</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...</td>\n",
       "      <td>['!!.']</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و #فسسسسسس</td>\n",
       "      <td>NEG</td>\n",
       "      <td>لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...</td>\n",
       "      <td>دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017838204,0.007064947,-0.03709342,-0.0264731...</td>\n",
       "      <td>0.205765</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...</td>\n",
       "      <td>['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>.</td>\n",
       "      <td>منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...</td>\n",
       "      <td>0.026780926,0.009709039,-0.030822175,-0.033138...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.237325</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...</td>\n",
       "      <td>POS</td>\n",
       "      <td>.</td>\n",
       "      <td># السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...</td>\n",
       "      <td>0.011575715,-0.0191376,-0.041333534,-0.0137087...</td>\n",
       "      <td>0.022658788,-0.0036188036,-0.033782676,-0.0437...</td>\n",
       "      <td>0.352307</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...</td>\n",
       "      <td>['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...</td>\n",
       "      <td>['.']</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>NEG</td>\n",
       "      <td>في الشوارع.</td>\n",
       "      <td>درية شرف الدين امرأة على الوشين لا مهنية ولا ا...</td>\n",
       "      <td>0.016909812,0.015640503,-0.02446039,-0.0235670...</td>\n",
       "      <td>0.017580768,-0.0027376027,-0.03825421,-0.04189...</td>\n",
       "      <td>0.390541</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...</td>\n",
       "      <td>['في', 'الشوارع.']</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...   NEG  \\\n",
       "1          دباسم يوسف عمل برنامج البرنامج و #فسسسسسس   NEG   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   NEG   \n",
       "3  #السعاده ان يكون من نحب بخير وعافيه فنحن نشعر ...   POS   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   NEG   \n",
       "\n",
       "                                            new_text   \n",
       "0                                                !!.  \\\n",
       "1  لر على # الفيس _ بوك [رابط]بسم الله الرحمن الر...   \n",
       "2                                                  .   \n",
       "3                                                  .   \n",
       "4                                        في الشوارع.   \n",
       "\n",
       "                                            all_text   \n",
       "0  5 هاتلي اخوان أي حاجة مش تنوين ومش ضمير اخوان ...  \\\n",
       "1  دباسم يوسف عمل برنامج البرنامج و # فسسلر على #...   \n",
       "2  منذ عامين وحتى الآن كل ما قدمه أنصار تيارات ال...   \n",
       "3  # السعاده ان يكون من نحب بخير وعافيه فنحن نشعر...   \n",
       "4  درية شرف الدين امرأة على الوشين لا مهنية ولا ا...   \n",
       "\n",
       "                                 original_embbedding   \n",
       "0  0.014882844,-0.051557414,-0.028316082,0.014168...  \\\n",
       "1  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "2  0.026780926,0.009709039,-0.030822175,-0.033138...   \n",
       "3  0.011575715,-0.0191376,-0.041333534,-0.0137087...   \n",
       "4  0.016909812,0.015640503,-0.02446039,-0.0235670...   \n",
       "\n",
       "                                      new_embbedding  ecu_similarity   \n",
       "0  0.01946623,-0.010952667,-0.039843258,-0.057320...        0.772223  \\\n",
       "1  0.017838204,0.007064947,-0.03709342,-0.0264731...        0.205765   \n",
       "2  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.237325   \n",
       "3  0.022658788,-0.0036188036,-0.033782676,-0.0437...        0.352307   \n",
       "4  0.017580768,-0.0027376027,-0.03825421,-0.04189...        0.390541   \n",
       "\n",
       "   cos_similarity  jacc_similarity   \n",
       "0           0.446         0.037037  \\\n",
       "1           0.929         0.437500   \n",
       "2           0.904         0.000000   \n",
       "3           0.829         0.000000   \n",
       "4           0.834         0.360000   \n",
       "\n",
       "                                          text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#السعاده', 'ان', 'يكون', 'من', 'نحب', 'بخير'...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      all_text_split   \n",
       "0  ['5', 'هاتلي', 'اخوان', 'أي', 'حاجة', 'مش', 'ت...  \\\n",
       "1  ['دباسم', 'يوسف', 'عمل', 'برنامج', 'البرنامج',...   \n",
       "2  ['منذ', 'عامين', 'وحتى', 'الآن', 'كل', 'ما', '...   \n",
       "3  ['#', 'السعاده', 'ان', 'يكون', 'من', 'نحب', 'ب...   \n",
       "4  ['درية', 'شرف', 'الدين', 'امرأة', 'على', 'الوش...   \n",
       "\n",
       "                                      new_text_split  bleu_sim_1  bleu_sim_2   \n",
       "0                                            ['!!.']        0.89        0.89  \\\n",
       "1  ['لر', 'على', '#', 'الفيس', '_', 'بوك', '[رابط...        0.14        0.13   \n",
       "2                                              ['.']        0.95        0.95   \n",
       "3                                              ['.']        0.79        0.78   \n",
       "4                                 ['في', 'الشوارع.']        0.92        0.92   \n",
       "\n",
       "   bleu_sim_3  bleu_sim_4  \n",
       "0        0.88        0.88  \n",
       "1        0.12        0.11  \n",
       "2        0.95        0.95  \n",
       "3        0.78        0.77  \n",
       "4        0.92        0.92  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetname = 'ASTD'\n",
    "datasetpath = \"Augmented-Dataset/xls/ASTD-Unbalanced-Augmented-aragpt2-base.xlsx\"\n",
    "df = pd.read_excel( datasetpath)\n",
    "df.columns = ['text', 'label', 'new_text', 'all_text', 'original_embbedding', 'new_embbedding', 'ecu_similarity', 'cos_similarity', 'jacc_similarity','text_split', 'all_text_split', 'new_text_split', 'bleu_sim_1','bleu_sim_2', 'bleu_sim_3', 'bleu_sim_4'] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "017d22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN  = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "all_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1190f",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dc34062",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "051195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Dataset - all text\n",
    "df = df[[DATA_COLUMN, LABEL_COLUMN]]\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "label_list = list(df[LABEL_COLUMN].unique())\n",
    "data = CustomDataset(datasetname+\"-Not-Augmented-all-text\", train, test, label_list)\n",
    "all_datasets.append(data)\n",
    "\n",
    "# Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-all-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-all-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-all-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-all-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf3363",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14909da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83535fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Augmented-Test-new-text\", train_ECU, test_ECU, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Augmented-Test-new-text\", train_COS, test_COS, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Augmented-Test-new-text\", train_JACC, test_JACC, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Augmented-Test-new-text\", train_BLEU, test_BLEU, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b294d0",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: All-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e8a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-ECU-ALL-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-COS-ALL-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-JAC-ALL-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/All/\"+datasetname+\"-Augmented-BLE-ALL-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0fae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Augmented-ECU-Final - all text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - all text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - all text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-all-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58ba",
   "metadata": {},
   "source": [
    "### Train: Augmented, Test: Not-Augmented, Text: New-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a02627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EcuDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-ECU-new-Text-Final.xlsx\")\n",
    "CosDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-COS-new-Text-Final.xlsx\")\n",
    "JacDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-JAC-new-Text-Final.xlsx\")\n",
    "BleDF = pd.read_excel( \"Augmented-Dataset/new/\"+datasetname+\"-Augmented-BLE-new-Text-Final.xlsx\")\n",
    "EcuDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "CosDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "JacDF.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "BleDF.columns = [DATA_COLUMN, LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83a9e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented-ECU-Final - new text \n",
    "train_ECU, test_ECU = train_test_split(EcuDF, test_size=0.2, random_state=42)\n",
    "label_list_ECU = list(EcuDF[LABEL_COLUMN].unique())\n",
    "data_ECU = CustomDataset(\"ECU-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_ECU, test, label_list_ECU)\n",
    "all_datasets.append(data_ECU)\n",
    "\n",
    "# Augmented-COS-Final - new text\n",
    "train_COS, test_COS = train_test_split(CosDF, test_size=0.2, random_state=42)\n",
    "label_list_COS = list(CosDF[LABEL_COLUMN].unique())\n",
    "data_COS = CustomDataset(\"COS-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_COS, test, label_list_COS)\n",
    "all_datasets.append(data_COS)\n",
    "\n",
    "# Augmented-JACC-Final - new text\n",
    "train_JACC, test_JACC = train_test_split(JacDF, test_size=0.2, random_state=42)\n",
    "label_list_JACC = list(JacDF[LABEL_COLUMN].unique())\n",
    "data_JACC = CustomDataset(\"JAC-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_JACC, test, label_list_JACC)\n",
    "all_datasets.append(data_JACC)\n",
    "\n",
    "# Augmented-BLEU-Final - all text\n",
    "train_BLEU, test_BLEU = train_test_split(BleDF, test_size=0.2, random_state=42)\n",
    "label_list_BLEU = list(BleDF[LABEL_COLUMN].unique())\n",
    "data_BLEU = CustomDataset(\"BLE-\"+datasetname+\"-Not-Augmented-Test-new-text\", train_BLEU, test, label_list_BLEU)\n",
    "all_datasets.append(data_BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4ecc7",
   "metadata": {},
   "source": [
    "### Printing All Datasets Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32d16716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTD-Not-Augmented-all-text\n",
      "ECU-ASTD-Augmented-Test-all-text\n",
      "COS-ASTD-Augmented-Test-all-text\n",
      "JAC-ASTD-Augmented-Test-all-text\n",
      "BLE-ASTD-Augmented-Test-all-text\n",
      "ECU-ASTD-Augmented-Test-new-text\n",
      "COS-ASTD-Augmented-Test-new-text\n",
      "JAC-ASTD-Augmented-Test-new-text\n",
      "BLE-ASTD-Augmented-Test-new-text\n",
      "ECU-ASTD-Not-Augmented-Test-all-text\n",
      "COS-ASTD-Not-Augmented-Test-all-text\n",
      "JAC-ASTD-Not-Augmented-Test-all-text\n",
      "BLE-ASTD-Not-Augmented-Test-all-text\n",
      "ECU-ASTD-Not-Augmented-Test-new-text\n",
      "COS-ASTD-Not-Augmented-Test-new-text\n",
      "JAC-ASTD-Not-Augmented-Test-new-text\n",
      "BLE-ASTD-Not-Augmented-Test-new-text\n"
     ]
    }
   ],
   "source": [
    "for d in all_datasets:\n",
    "    print(d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c05ff5e",
   "metadata": {},
   "source": [
    "### PR-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3ed0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "datasetname         = 'ASTD'\n",
    "model_name          = 'aubmindlab/bert-base-arabertv02-twitter' \n",
    "all_models_dir      = \"models\\\\\" + datasetname + \"\\\\\"\n",
    "all_results         = []\n",
    "positive_label_key  = 1 # {'NEG': 0, 'POS': 1, 'NEUTRAL': 2}\n",
    "\n",
    "# ** find_folders_with_prefix **\n",
    "def find_folders_with_prefix(directory, search_str):\n",
    "    folders = []\n",
    "    for item in os.listdir(directory):\n",
    "        if os.path.isdir(os.path.join(directory, item)) and item.startswith(search_str):\n",
    "            folders.append(item)\n",
    "    return folders\n",
    "\n",
    "# ** compute_roc_and_pr **    \n",
    "def compute_roc_and_pr (true_labesl, predicted_labels, pos_label):\n",
    "    precision, recall, _ = precision_recall_curve (true_labels, predicted_labels, pos_label=pos_label) \n",
    "    pr_auc               = auc                    (recall, precision)\n",
    "    fpr, tpr, _          = roc_curve              (true_labels, predicted_labels, pos_label=pos_label)\n",
    "    roc_auc              = auc                    (fpr, tpr)\n",
    "   \n",
    "    return {    'fpr'           : fpr            ,\n",
    "                'tpr'           : tpr            ,\n",
    "                'precision'     : precision      ,\n",
    "                'recall'        : recall         ,\n",
    "                'roc_auc'       : roc_auc        ,\n",
    "                'pr_auc'        : pr_auc\n",
    "           }\n",
    "\n",
    "# ** get_fold_num_from_str ** \n",
    "def get_fold_num_from_str (text):\n",
    "    tmp = -1    \n",
    "    if '_0_' in text:\n",
    "        tmp = 0\n",
    "    elif '_1_' in text:\n",
    "        tmp = 1\n",
    "    elif '_2_' in text:\n",
    "        tmp = 2\n",
    "    elif '_3_' in text:\n",
    "        tmp = 3\n",
    "    elif '_4_' in text:\n",
    "        tmp = 4\n",
    "    else:\n",
    "        return -1    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3abae51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR&ROC: Dataset-ASTD-Not-Augmented-all-text, fold-0\n",
      "PR&ROC: Dataset-ASTD-Not-Augmented-all-text, fold-1\n",
      "PR&ROC: Dataset-ASTD-Not-Augmented-all-text, fold-2\n",
      "PR&ROC: Dataset-ASTD-Not-Augmented-all-text, fold-3\n",
      "PR&ROC: Dataset-ASTD-Not-Augmented-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ASTD-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-COS-ASTD-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ASTD-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ASTD-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-all-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-all-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-all-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-all-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-all-text, fold-4\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-ECU-ASTD-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-COS-ASTD-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-JAC-ASTD-Not-Augmented-Test-new-text, fold-4\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-new-text, fold-0\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-new-text, fold-1\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-new-text, fold-2\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-new-text, fold-3\n",
      "PR&ROC: Dataset-BLE-ASTD-Not-Augmented-Test-new-text, fold-4\n"
     ]
    }
   ],
   "source": [
    "# process all datasets for finding PR and ROC    \n",
    "# ['NEG', 'POS', 'NEUTRAL'] ******************************************\n",
    "for dataset in all_datasets:       \n",
    "    selected_dataset                = copy.deepcopy(dataset)\n",
    "    dataset_name                    = selected_dataset.name\n",
    "    dataset_models_dirs             = []\n",
    "    arabic_prep                     = ArabertPreprocessor(model_name)\n",
    "    label_map                       = {v:index for index, v in enumerate(selected_dataset.label_list)}        \n",
    "    fold_num                        = 0\n",
    "    dataset_models_dirs             = find_folders_with_prefix(all_models_dir, dataset_name)\n",
    "    for directory in dataset_models_dirs:\n",
    "        fold_num     = get_fold_num_from_str(directory)\n",
    "        print(\"PR&ROC: Dataset-\" + dataset_name + \", fold-\" + str(fold_num))\n",
    "        model_dir    = all_models_dir + directory\n",
    "        config_dir   = all_models_dir + directory + \"\\\\config.json\"\n",
    "        model        = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "        config       = AutoConfig.from_pretrained(config_dir)\n",
    "        tokenizer    = AutoTokenizer.from_pretrained(model_name)\n",
    "         \n",
    "        # Use DataLoader to process the data in batches\n",
    "        test_data    = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x)).to_list()\n",
    "        test_labels  = selected_dataset.test[LABEL_COLUMN].to_list()\n",
    "        test_dataset = list(zip(test_data, test_labels))\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "         \n",
    "        predicted_probabilities = []\n",
    "        true_labels             = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                 #print(batch)\n",
    "                batch_data, label_data = batch #batch_data   = [data for data, _ in batch]\n",
    "                batch_labels           = [label_map[label] for label in label_data]\n",
    "                inputs                 = tokenizer(batch_data, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                outputs                = model(**inputs)\n",
    "                batch_probs            = outputs.logits.softmax(dim=-1).detach().numpy()\n",
    "                predicted_probabilities.append(batch_probs)\n",
    "                true_labels.extend(batch_labels)\n",
    "                \n",
    "        predicted_probabilities = np.concatenate(predicted_probabilities, axis=0)\n",
    "        predicted_labels        = predicted_probabilities.argmax(axis=-1)\n",
    "        results                 = compute_roc_and_pr(true_labels, predicted_labels, positive_label_key)\n",
    "        results['Dataset_Name'] = dataset_name\n",
    "        results['Fold_No']      = fold_num\n",
    "        all_results.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec9af33",
   "metadata": {},
   "source": [
    "### Export Results to Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "369ddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(all_results, orient='columns')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bb0ca21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Fold_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.006, 0.134, 1.0]</td>\n",
       "      <td>[0.0, 0.013793103448275862, 0.6275862068965518...</td>\n",
       "      <td>[0.2248062015503876, 0.5759493670886076, 0.4, ...</td>\n",
       "      <td>[1.0, 0.6275862068965518, 0.013793103448275862...</td>\n",
       "      <td>0.745834</td>\n",
       "      <td>0.458277</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.004, 0.12, 1.0]</td>\n",
       "      <td>[0.0, 0.006896551724137931, 0.6344827586206897...</td>\n",
       "      <td>[0.2248062015503876, 0.6052631578947368, 0.333...</td>\n",
       "      <td>[1.0, 0.6344827586206897, 0.006896551724137931...</td>\n",
       "      <td>0.756386</td>\n",
       "      <td>0.450825</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.012, 0.122, 1.0]</td>\n",
       "      <td>[0.0, 0.027586206896551724, 0.6275862068965518...</td>\n",
       "      <td>[0.2248062015503876, 0.5986842105263158, 0.4, ...</td>\n",
       "      <td>[1.0, 0.6275862068965518, 0.027586206896551724...</td>\n",
       "      <td>0.750710</td>\n",
       "      <td>0.472255</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.006, 0.124, 1.0]</td>\n",
       "      <td>[0.0, 0.006896551724137931, 0.6, 1.0]</td>\n",
       "      <td>[0.2248062015503876, 0.5838926174496645, 0.25,...</td>\n",
       "      <td>[1.0, 0.6, 0.006896551724137931, 0.0]</td>\n",
       "      <td>0.736628</td>\n",
       "      <td>0.413342</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.012, 0.122, 1.0]</td>\n",
       "      <td>[0.0, 0.020689655172413793, 0.593103448275862,...</td>\n",
       "      <td>[0.2248062015503876, 0.5850340136054422, 0.333...</td>\n",
       "      <td>[1.0, 0.593103448275862, 0.020689655172413793,...</td>\n",
       "      <td>0.733255</td>\n",
       "      <td>0.441397</td>\n",
       "      <td>ASTD-Not-Augmented-all-text</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[0.0, 0.062, 0.184, 1.0]</td>\n",
       "      <td>[0.0, 0.07586206896551724, 0.7724137931034483,...</td>\n",
       "      <td>[0.2248062015503876, 0.5490196078431373, 0.261...</td>\n",
       "      <td>[1.0, 0.7724137931034483, 0.07586206896551724,...</td>\n",
       "      <td>0.777241</td>\n",
       "      <td>0.418347</td>\n",
       "      <td>BLE-ASTD-Not-Augmented-Test-new-text</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>[0.0, 0.052, 0.168, 1.0]</td>\n",
       "      <td>[0.0, 0.06896551724137931, 0.7310344827586207,...</td>\n",
       "      <td>[0.2248062015503876, 0.5578947368421052, 0.277...</td>\n",
       "      <td>[1.0, 0.7310344827586207, 0.06896551724137931,...</td>\n",
       "      <td>0.768303</td>\n",
       "      <td>0.425958</td>\n",
       "      <td>BLE-ASTD-Not-Augmented-Test-new-text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[0.0, 0.064, 0.198, 1.0]</td>\n",
       "      <td>[0.0, 0.05517241379310345, 0.7655172413793103,...</td>\n",
       "      <td>[0.2248062015503876, 0.5285714285714286, 0.2, ...</td>\n",
       "      <td>[1.0, 0.7655172413793103, 0.05517241379310345,...</td>\n",
       "      <td>0.764724</td>\n",
       "      <td>0.380199</td>\n",
       "      <td>BLE-ASTD-Not-Augmented-Test-new-text</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>[0.0, 0.07, 0.194, 1.0]</td>\n",
       "      <td>[0.0, 0.06206896551724138, 0.7862068965517242,...</td>\n",
       "      <td>[0.2248062015503876, 0.5402843601895735, 0.204...</td>\n",
       "      <td>[1.0, 0.7862068965517242, 0.06206896551724138,...</td>\n",
       "      <td>0.774607</td>\n",
       "      <td>0.388848</td>\n",
       "      <td>BLE-ASTD-Not-Augmented-Test-new-text</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>[0.0, 0.048, 0.178, 1.0]</td>\n",
       "      <td>[0.0, 0.05517241379310345, 0.7724137931034483,...</td>\n",
       "      <td>[0.2248062015503876, 0.5572139303482587, 0.25,...</td>\n",
       "      <td>[1.0, 0.7724137931034483, 0.05517241379310345,...</td>\n",
       "      <td>0.783579</td>\n",
       "      <td>0.412955</td>\n",
       "      <td>BLE-ASTD-Not-Augmented-Test-new-text</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fpr   \n",
       "0   [0.0, 0.006, 0.134, 1.0]  \\\n",
       "1    [0.0, 0.004, 0.12, 1.0]   \n",
       "2   [0.0, 0.012, 0.122, 1.0]   \n",
       "3   [0.0, 0.006, 0.124, 1.0]   \n",
       "4   [0.0, 0.012, 0.122, 1.0]   \n",
       "..                       ...   \n",
       "80  [0.0, 0.062, 0.184, 1.0]   \n",
       "81  [0.0, 0.052, 0.168, 1.0]   \n",
       "82  [0.0, 0.064, 0.198, 1.0]   \n",
       "83   [0.0, 0.07, 0.194, 1.0]   \n",
       "84  [0.0, 0.048, 0.178, 1.0]   \n",
       "\n",
       "                                                  tpr   \n",
       "0   [0.0, 0.013793103448275862, 0.6275862068965518...  \\\n",
       "1   [0.0, 0.006896551724137931, 0.6344827586206897...   \n",
       "2   [0.0, 0.027586206896551724, 0.6275862068965518...   \n",
       "3               [0.0, 0.006896551724137931, 0.6, 1.0]   \n",
       "4   [0.0, 0.020689655172413793, 0.593103448275862,...   \n",
       "..                                                ...   \n",
       "80  [0.0, 0.07586206896551724, 0.7724137931034483,...   \n",
       "81  [0.0, 0.06896551724137931, 0.7310344827586207,...   \n",
       "82  [0.0, 0.05517241379310345, 0.7655172413793103,...   \n",
       "83  [0.0, 0.06206896551724138, 0.7862068965517242,...   \n",
       "84  [0.0, 0.05517241379310345, 0.7724137931034483,...   \n",
       "\n",
       "                                            precision   \n",
       "0   [0.2248062015503876, 0.5759493670886076, 0.4, ...  \\\n",
       "1   [0.2248062015503876, 0.6052631578947368, 0.333...   \n",
       "2   [0.2248062015503876, 0.5986842105263158, 0.4, ...   \n",
       "3   [0.2248062015503876, 0.5838926174496645, 0.25,...   \n",
       "4   [0.2248062015503876, 0.5850340136054422, 0.333...   \n",
       "..                                                ...   \n",
       "80  [0.2248062015503876, 0.5490196078431373, 0.261...   \n",
       "81  [0.2248062015503876, 0.5578947368421052, 0.277...   \n",
       "82  [0.2248062015503876, 0.5285714285714286, 0.2, ...   \n",
       "83  [0.2248062015503876, 0.5402843601895735, 0.204...   \n",
       "84  [0.2248062015503876, 0.5572139303482587, 0.25,...   \n",
       "\n",
       "                                               recall   roc_auc    pr_auc   \n",
       "0   [1.0, 0.6275862068965518, 0.013793103448275862...  0.745834  0.458277  \\\n",
       "1   [1.0, 0.6344827586206897, 0.006896551724137931...  0.756386  0.450825   \n",
       "2   [1.0, 0.6275862068965518, 0.027586206896551724...  0.750710  0.472255   \n",
       "3               [1.0, 0.6, 0.006896551724137931, 0.0]  0.736628  0.413342   \n",
       "4   [1.0, 0.593103448275862, 0.020689655172413793,...  0.733255  0.441397   \n",
       "..                                                ...       ...       ...   \n",
       "80  [1.0, 0.7724137931034483, 0.07586206896551724,...  0.777241  0.418347   \n",
       "81  [1.0, 0.7310344827586207, 0.06896551724137931,...  0.768303  0.425958   \n",
       "82  [1.0, 0.7655172413793103, 0.05517241379310345,...  0.764724  0.380199   \n",
       "83  [1.0, 0.7862068965517242, 0.06206896551724138,...  0.774607  0.388848   \n",
       "84  [1.0, 0.7724137931034483, 0.05517241379310345,...  0.783579  0.412955   \n",
       "\n",
       "                            Dataset_Name  Fold_No  \n",
       "0            ASTD-Not-Augmented-all-text        0  \n",
       "1            ASTD-Not-Augmented-all-text        1  \n",
       "2            ASTD-Not-Augmented-all-text        2  \n",
       "3            ASTD-Not-Augmented-all-text        3  \n",
       "4            ASTD-Not-Augmented-all-text        4  \n",
       "..                                   ...      ...  \n",
       "80  BLE-ASTD-Not-Augmented-Test-new-text        0  \n",
       "81  BLE-ASTD-Not-Augmented-Test-new-text        1  \n",
       "82  BLE-ASTD-Not-Augmented-Test-new-text        2  \n",
       "83  BLE-ASTD-Not-Augmented-Test-new-text        3  \n",
       "84  BLE-ASTD-Not-Augmented-Test-new-text        4  \n",
       "\n",
       "[85 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "761f6a44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainResults\u001b[49m\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatestResults/ASTD/ASTD-PR-ROC.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainResults' is not defined"
     ]
    }
   ],
   "source": [
    "trainResults.to_excel(\"LatestResults/ASTD/ASTD-PR-ROC.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a40f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
